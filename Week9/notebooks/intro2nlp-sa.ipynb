{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<DIV ALIGN=CENTER>\n",
    "\n",
    "# Introduction to NLP: Semantic Analysis\n",
    "## Professor Robert J. Brunner\n",
    "  \n",
    "</DIV>  \n",
    "-----\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Introduction\n",
    "\n",
    "One of the most elusive goals in natural language processing is the\n",
    "identification of semantic meaning from text. In this case, we don't\n",
    "want to simply use text to identify topics or classify documents.\n",
    "Instead we want to use the relationships between words in documents to\n",
    "gain insight from context. One simple way to understand this concept is\n",
    "to see that when words occur in similar arrangements or patterns across\n",
    "documents, the pattern conveys meaning beyond the simple appearance of\n",
    "the words themselves. \n",
    "\n",
    "In this notebook, we explore semantic analysis. First we use the\n",
    "wordnet corpus to identify similar words based on pre-defined\n",
    "relationships. Second, we use the gensim library to create topic models\n",
    "that can be used to compute similarity measures based on the inherent\n",
    "patterns of words within a corpus. Finally, we explore the word2vec\n",
    "approach, where a neural network is trained on a large corpus to\n",
    "identify relationships between words and phrase.\n",
    "\n",
    "-----\n",
    "\n",
    "## Wordnet\n",
    "\n",
    "[Wordnet][wdn] is an English lexical database that groups words into\n",
    "synsets, which is shorthand for synonym sets. The database was created\n",
    "under at Princeton University and has been distributed with an open\n",
    "license. Given its nature, it is a different corpus than the Treebank or\n",
    "Brown corpora analyzed in the [Topic Modeling][l2] notebook. A wordnet\n",
    "entry can have multiple definitions for a word, associated synonyms,\n",
    "lemmas, and other information that can be used to algorithmically\n",
    "identify relationships between words. In the next few code cells we\n",
    "explore the wordnet corpus, before moving on to using it to compute word\n",
    "similarities.\n",
    "\n",
    "-----\n",
    "[wdn]: https://en.wikipedia.org/wiki/WordNet \n",
    "[l2]: intro2nlp-tm.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34 total entries in synonym ring for drive. Only showing top 5 entries.\n",
      "----------------------------------------------------------------------\n",
      "drive (n): the act of applying force to propel something\n",
      "----------------------------------------------------------------------\n",
      "drive (n): a mechanism by which force or power is transmitted in a machine\n",
      "----------------------------------------------------------------------\n",
      "campaign (n): a series of actions advancing a principle or tending toward a particular end\n",
      "----------------------------------------------------------------------\n",
      "driveway (n): a road leading up to a private house\n",
      "----------------------------------------------------------------------\n",
      "drive (n): the trait of being highly motivated\n",
      "----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Explore WordNet synonym rings\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "# We limit the number of items in ring to display\n",
    "max_entries = 5\n",
    "\n",
    "# Choose a word, change this to see different results.\n",
    "the_word = 'drive'\n",
    "the_synsets = wn.synsets(the_word)\n",
    "\n",
    "# Display summary stats\n",
    "print('{0} total entries in synonym ring for {1}. '.format(len(the_synsets), the_word), end='')\n",
    "print('Only showing top {0} entries.'.format(max_entries))\n",
    "print(70*'-')\n",
    "\n",
    "# Step through ring and display data\n",
    "for synset in the_synsets[:max_entries]:\n",
    "    vals = synset.name().split('.')\n",
    "    print('{0} ({1}): '.format(vals[0], vals[1]), end='')\n",
    "    print(synset.definition())\n",
    "    print(70*'-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drive (n): Example: after reaching the desired velocity the drive is cut off\n",
      "    Lemma('drive.n.01.drive')\n",
      "    Lemma('drive.n.01.thrust')\n",
      "    Lemma('drive.n.01.driving_force')\n",
      "------------------------------------------------------------\n",
      "drive (n): Example: a variable speed drive permitted operation through a range of speeds\n",
      "    Lemma('drive.n.02.drive')\n",
      "------------------------------------------------------------\n",
      "campaign (n): Example: he supported populist campaigns\n",
      "    Lemma('campaign.n.02.campaign')\n",
      "    Lemma('campaign.n.02.cause')\n",
      "    Lemma('campaign.n.02.crusade')\n",
      "    Lemma('campaign.n.02.drive')\n",
      "    Lemma('campaign.n.02.movement')\n",
      "    Lemma('campaign.n.02.effort')\n",
      "------------------------------------------------------------\n",
      "driveway (n): Example: they parked in the driveway\n",
      "    Lemma('driveway.n.01.driveway')\n",
      "    Lemma('driveway.n.01.drive')\n",
      "    Lemma('driveway.n.01.private_road')\n",
      "------------------------------------------------------------\n",
      "drive (n): Example: his drive and energy exhausted his co-workers\n",
      "    Lemma('drive.n.05.drive')\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Now we display the synonyms, definitions and lemmas.\n",
    "\n",
    "for synset in the_synsets[:max_entries]:\n",
    "    \n",
    "    vals = synset.name().split('.')\n",
    "    print('{0} ({1}): '.format(vals[0], vals[1]), end='')\n",
    "    if synset.examples():\n",
    "        print('Example: {0}'.format(synset.examples()[0]))\n",
    "        \n",
    "    for lma in synset.lemmas():\n",
    "        print('    {0}'.format(lma))\n",
    "\n",
    "    print(60*'-')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "### Word Similarities\n",
    "\n",
    "We can use wordnet to compute word similarities. The wordnet corpus has\n",
    "tagged tokens, which can be used to compute paths between two words.\n",
    "Shorter paths generally correspond to more similar words. In the\n",
    "following examples, we start with several simple wordnet synonym rings,\n",
    "and compute similarities between these words.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define some words\n",
    "man = wn.synset('man.n.01')\n",
    "woman = wn.synset('woman.n.01')\n",
    "horse = wn.synset('horse.n.01')\n",
    "bird = wn.synset('bird.n.01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path Similarity:\n",
      "----------------------------------------\n",
      "man to woman: 0.333\n",
      "man to horse: 0.077\n",
      "man to bird: 0.125\n",
      "woman to woman: 1.000\n"
     ]
    }
   ],
   "source": [
    "# Now we print similarity measures.\n",
    "fmt_str = '{1} to {2}: {0:4.3f}'\n",
    "\n",
    "print('Path Similarity:')\n",
    "print(40*'-')\n",
    "print(fmt_str.format(wn.path_similarity(man, woman), 'man', 'woman'))\n",
    "print(fmt_str.format(wn.path_similarity(man, horse), 'man', 'horse'))\n",
    "print(fmt_str.format(wn.path_similarity(man, bird), 'man', 'bird'))\n",
    "print(fmt_str.format(wn.path_similarity(woman, woman), 'woman', 'woman'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Leacock-Chodorow Similarity:\n",
      "----------------------------------------\n",
      "man to woman: 2.539\n",
      "man to horse: 1.073\n",
      "man to bird: 1.558\n",
      "woman to woman: 3.638\n"
     ]
    }
   ],
   "source": [
    "print('Leacock-Chodorow Similarity:')\n",
    "print(40*'-')\n",
    "print(fmt_str.format(wn.lch_similarity(man, woman), 'man', 'woman'))\n",
    "print(fmt_str.format(wn.lch_similarity(man, horse), 'man', 'horse'))\n",
    "print(fmt_str.format(wn.lch_similarity(man, bird), 'man', 'bird'))\n",
    "print(fmt_str.format(wn.lch_similarity(woman, woman), 'woman', 'woman'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wu-Palmer Similarity:\n",
      "----------------------------------------\n",
      "man to woman: 0.667\n",
      "man to horse: 0.500\n",
      "man to bird: 0.632\n",
      "woman to woman: 0.667\n"
     ]
    }
   ],
   "source": [
    "print('Wu-Palmer Similarity:')\n",
    "print(40*'-')\n",
    "print(fmt_str.format(wn.wup_similarity(man, woman), 'man', 'woman'))\n",
    "print(fmt_str.format(wn.wup_similarity(man, horse), 'man', 'horse'))\n",
    "print(fmt_str.format(wn.wup_similarity(man, bird), 'man', 'bird'))\n",
    "print(fmt_str.format(wn.wup_similarity(woman, woman), 'woman', 'woman'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "### Student Activity\n",
    "\n",
    "In the preceding cells, we used wordnet to compute word similarities.\n",
    "Now that you have run the Notebook, go back and make the following\n",
    "changes to see how the results change.\n",
    "\n",
    "1. Change the value of `the_word` to a different word, like _ship_ or\n",
    "_throw_. How many entries does the new word have in the wordnet synset?\n",
    "2. Compute the path similarity for a different set of words, like\n",
    "_cat_, _dog_, _boy, and _girl_.\n",
    "\n",
    "----------\n",
    "\n",
    "## Gensim\n",
    "\n",
    "We can use the topic models constructed by using the gensim library to\n",
    "look for similarity. In this case, we build the topic model and use\n",
    "these models to create a similarity matrix. By transforming new text\n",
    "into this vector space, we can compute similarity measures by using the\n",
    "model learned from the original text. In the following code cells, we\n",
    "build a topic model by using LDA from our course description text.\n",
    "Finally, we use this new model to compute similarity measures between\n",
    "the original text and new sample text.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# As a text example, we use the course description for INFO490  SP16.\n",
    "info_course = ['Advanced Data Science: This class is an asynchronous, online course.', \n",
    "               'This course will introduce advanced data science concepts by building on the foundational concepts presented in INFO 490: Foundations of Data Science.', \n",
    "               'Students will first learn how to perform more statistical data exploration and constructing and evaluating statistical models.', \n",
    "               'Next, students will learn machine learning techniques including supervised and unsupervised learning, dimensional reduction, and cluster finding.', \n",
    "               'An emphasis will be placed on the practical application of these techniques to high-dimensional numerical data, time series data, image data, and text data.', \n",
    "               'Finally, students will learn to use relational databases and cloud computing software components such as Hadoop, Spark, and NoSQL data stores.', \n",
    "               'Students must have access to a fairly modern computer, ideally that supports hardware virtualization, on which they can install software.', \n",
    "               'This class is open to sophomores, juniors, seniors and graduate students in any discipline who have either taken a previous INFO 490 data science course or have received instructor permission.']\n",
    "\n",
    "# Simple stop words\n",
    "stop_words = set('for a of the and to in on an'.split())\n",
    "\n",
    "# Parse text into words, make lowercase and remove stop words\n",
    "txts = [[word for word in sentance.lower().split() if word not in stop_words]\n",
    "        for sentance in info_course]\n",
    "\n",
    "# Keep only those words appearing more than once\n",
    "# Easy with a Counter, but need a flat list\n",
    "from collections import Counter\n",
    "frequency = Counter([word for txt in txts for word in txt])\n",
    "\n",
    "# Now grab tokens that appear more than once\n",
    "tokens = [[token for token in txt if frequency[token] > 1]\n",
    "          for txt in txts]\n",
    "\n",
    "from gensim import corpora\n",
    "dict_gensim = corpora.Dictionary(tokens)\n",
    "\n",
    "crps = [dict_gensim.doc2bow(txt) for txt in txts]\n",
    "\n",
    "from gensim import models\n",
    "\n",
    "tfidf = models.TfidfModel(crps)\n",
    "\n",
    "crps_tfidf = tfidf[crps]\n",
    "\n",
    "lda_gs = models.LdaModel(corpus=crps_tfidf, id2word=dict_gensim, num_topics=5, passes=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.066886094590185421), (1, 0.070131630706514278), (2, 0.068173353410123164), (3, 0.72727084865285241), (4, 0.067538072640324673)]\n"
     ]
    }
   ],
   "source": [
    "# Create bag of words vector space representation for sample text.\n",
    "\n",
    "txt = 'Master Data Science'\n",
    "vec_bow = dict_gensim.doc2bow(txt.lower().split())\n",
    "vec_lda = lda_gs[vec_bow]\n",
    "\n",
    "# Display BOW representation\n",
    "print(vec_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:gensim.similarities.docsim:scanning corpus to determine the number of features (consider setting `num_features` explicitly)\n"
     ]
    }
   ],
   "source": [
    "# Compute similarity matrix from the topic model\n",
    "\n",
    "from gensim import similarities\n",
    "index = similarities.MatrixSimilarity(lda_gs[crps_tfidf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.19779952,  0.99967492,  0.23596762,  0.21556196,  0.25915855,\n",
       "        0.21237224,  0.25491434,  0.17991626], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display sample text values from similarity matrix\n",
    "\n",
    "index[vec_lda]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Find similar sentances to given set of words.\n",
    "\n",
    "import operator\n",
    "\n",
    "def find_similar_sentances(mdl, dct, sml_idx, txt, prct = 0.9):\n",
    "    vec_bow = dct.doc2bow(txt.lower().split())\n",
    "    vec_lda = mdl[vec_bow]\n",
    "    \n",
    "    sml = sorted(enumerate(sml_idx[vec_lda]), \\\n",
    "                 key=operator.itemgetter(1), reverse=True)\n",
    "    print('Score| Text')\n",
    "    print(4*'-', '|', 73*'-')\n",
    "    \n",
    "    for idx, val in sml:\n",
    "        if val > prct:\n",
    "            print('{0:4.3f}: {1}'.format(val, info_course[idx]))\n",
    "            print(4*'-', '|', 73*'-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score| Text\n",
      "---- | -------------------------------------------------------------------------\n",
      "1.000: This course will introduce advanced data science concepts by building on the foundational concepts presented in INFO 490: Foundations of Data Science.\n",
      "---- | -------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "find_similar_sentances(lda_gs, dict_gensim, index, txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score| Text\n",
      "---- | -------------------------------------------------------------------------\n",
      "0.996: Students will first learn how to perform more statistical data exploration and constructing and evaluating statistical models.\n",
      "---- | -------------------------------------------------------------------------\n",
      "0.992: Next, students will learn machine learning techniques including supervised and unsupervised learning, dimensional reduction, and cluster finding.\n",
      "---- | -------------------------------------------------------------------------\n",
      "0.991: Finally, students will learn to use relational databases and cloud computing software components such as Hadoop, Spark, and NoSQL data stores.\n",
      "---- | -------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "txt = 'evaluate statistical plots'\n",
    "find_similar_sentances(lda_gs, dict_gensim, index, txt, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score| Text\n",
      "---- | -------------------------------------------------------------------------\n",
      "0.996: Students will first learn how to perform more statistical data exploration and constructing and evaluating statistical models.\n",
      "---- | -------------------------------------------------------------------------\n",
      "0.992: Next, students will learn machine learning techniques including supervised and unsupervised learning, dimensional reduction, and cluster finding.\n",
      "---- | -------------------------------------------------------------------------\n",
      "0.991: Finally, students will learn to use relational databases and cloud computing software components such as Hadoop, Spark, and NoSQL data stores.\n",
      "---- | -------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "txt = 'learn computing'\n",
    "find_similar_sentances(lda_gs, dict_gensim, index, txt, 0.75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "### Student Activity\n",
    "\n",
    "In the preceding cells, we build an LDA model from our course\n",
    "description text. Now that you have run the Notebook, go back and make\n",
    "the following changes to see how the results change.\n",
    "\n",
    "1. Try using a different set of words, do the similar sentences make\n",
    "sense? Can you explain why?\n",
    "\n",
    "2. Try using a different corpus (like the twenty newsgroup data set) to\n",
    "build the LDA model. Do word similarities make more or less sense with\n",
    "this new model\n",
    "\n",
    "\n",
    "-----\n",
    "\n",
    "## Word2Vec\n",
    "\n",
    "Word2vec is a neural network model that was developed several years ago\n",
    "within Google to provide an efficient continuous bag of words and\n",
    "skip-gram algorithms for word-vector representations. By using these\n",
    "approaches, words can be directly compared by using their vector\n",
    "representations to compute similarity measures. The continuous bag of\n",
    "words can be used to predict a context given a word, while a skip gram\n",
    "can be used to predict a word given a context. In this notebook we use\n",
    "the word2vec implementation provided in the gensim library. We first\n",
    "create the model, in this case we start with the parsed course\n",
    "description. Given this model, we can compute word similarities.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "\n",
    "model = gensim.models.Word2Vec(txts, window=2, min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1.000: data to data\n",
      " 0.157: data to science\n",
      "-0.175: image to data\n",
      " 0.055: students to computing\n"
     ]
    }
   ],
   "source": [
    "# Compute cosine similarity between two words.\n",
    "\n",
    "def get_similarity(mdl, w1, w2):\n",
    "    sml = mdl.similarity(w1, w2)\n",
    "    print('{0:6.3f}: {1} to {2}'.format(sml, w1, w2))\n",
    "\n",
    "get_similarity(model, 'data', 'data')\n",
    "get_similarity(model, 'data', 'science')\n",
    "get_similarity(model, 'image', 'data')\n",
    "get_similarity(model, 'students', 'computing')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "The previous example demonstrated how to use _word2vec_ by using the\n",
    "gensim library. But given the small size of the text document, this\n",
    "didn't really demonstrate the full power of this approach. We now switch\n",
    "to the NLTK movie review corpus, and build a word2vec model from this\n",
    "text data. First, we read the data into the notebook, before tokenizing\n",
    "the data and building the vector space model. Given the large number of\n",
    "words in this corpus, we can compute similarities between a larger\n",
    "number of words, as well as explore relationships between words, all\n",
    "based on the relative occurrences of words in the original corpus.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load movie review corpus\n",
    "import nltk\n",
    "mvr = nltk.corpus.movie_reviews\n",
    "\n",
    "from sklearn.datasets import load_files\n",
    "\n",
    "data_dir = '/home/data_scientist/data/nltk_data/corpora/movie_reviews'\n",
    "mvr = load_files(data_dir, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of reviews in Corpus = 2000\n"
     ]
    }
   ],
   "source": [
    "# Tokenize movie reviews by using a word-punctuation tokenizer\n",
    "\n",
    "import string\n",
    "\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "tokenizer = WordPunctTokenizer()\n",
    "\n",
    "new_mvr = []\n",
    "\n",
    "# We explicitly cull out punctuation tokens\n",
    "for mvr in mvr.data:\n",
    "    wtks = tokenizer.tokenize(mvr.decode('utf-8'))\n",
    "    new_mvr.append([wtk for wtk in wtks if wtk not in string.punctuation])\n",
    "\n",
    "print('Number of reviews in Corpus = {0}'.format(len(new_mvr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build word2vec model from movie reviews\n",
    "model = gensim.models.Word2Vec(new_mvr, window=5, min_count=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1.000: girl to girl\n",
      " 0.828: boy to girl\n",
      " 0.837: woman to man\n",
      " 0.819: woman to girl\n"
     ]
    }
   ],
   "source": [
    "# Compute Cosine Similarities from Corpus\n",
    "get_similarity(model, 'girl', 'girl')\n",
    "get_similarity(model, 'boy', 'girl')\n",
    "get_similarity(model, 'woman', 'man')\n",
    "get_similarity(model, 'woman', 'girl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# simple function to display words that are similar to a given word\n",
    "def show_words(vals, type='Cosine Similarity'):\n",
    "    print('{0:14s}: {1}'.format('Word', type))\n",
    "    print(40*'-')\n",
    "    for val in vals:\n",
    "        print('{0:14s}: {1:6.3f}'.format(val[0], val[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word          : Cosine Similarity\n",
      "----------------------------------------\n",
      "girl          :  0.828\n",
      "woman         :  0.744\n",
      "man           :  0.735\n",
      "immigrant     :  0.704\n",
      "osment        :  0.687\n"
     ]
    }
   ],
   "source": [
    "#Compute cosine similarity between two words.\n",
    "\n",
    "vals = model.most_similar('boy', topn=5)\n",
    "show_words(vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'boy'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Identify words that don't belong\n",
    "\n",
    "wrd_lst = ['boy', 'horse', 'cow', 'pig']\n",
    "\n",
    "model.doesnt_match(wrd_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity =  0.811\n"
     ]
    }
   ],
   "source": [
    "# Compute cosine similarity between two sets of words.\n",
    "\n",
    "val = model.n_similarity(['boy', 'girl'], ['man', 'woman'])\n",
    "\n",
    "print('Cosine Similarity = {0:6.3f}'.format(val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word          : Cosine Similarity\n",
      "----------------------------------------\n",
      "fat           :  0.727\n",
      "attendant     :  0.675\n",
      "deceased      :  0.672\n",
      "monk          :  0.664\n",
      "boy           :  0.660\n"
     ]
    }
   ],
   "source": [
    "# Find similar words (Cosine Similarity)\n",
    "\n",
    "vals = model.most_similar(positive=['woman', 'girl'], negative=['man'], topn=5)\n",
    "show_words(vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word          : CosMul Similarity\n",
      "----------------------------------------\n",
      "oscar         :  0.821\n",
      "sommers       :  0.819\n",
      "actress       :  0.818\n",
      "performance   :  0.812\n",
      "raja          :  0.785\n"
     ]
    }
   ],
   "source": [
    "# Find similar words (Multiplicative Combination method)\n",
    "\n",
    "vals = model.most_similar_cosmul('actor', topn=5)\n",
    "show_words(vals, 'CosMul Similarity')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "### Student Activity\n",
    "\n",
    "In the preceding cells, we applied the word2vec model to the movie\n",
    "review data set. Before proceeding, do the results make sense (feel free\n",
    "to discuss in the class forums). Now that you have run the Notebook, go\n",
    "back and make the following changes to see how the results change.\n",
    "\n",
    "1. Change the `count` and `window` values. How do the results change?\n",
    "2. Try exploring the relationship between other words, like _cat_,\n",
    "_dog_, _bird_, and _horse_; or other word combinations that are likely\n",
    "to appear in movie reviews.\n",
    "3. Can you apply word2vec to a different corpus, like the Brown corpus\n",
    "in NLTK? How do the similarity measures change with this new corpus for\n",
    "the same set of words?\n",
    "\n",
    "-----"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
