{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<DIV ALIGN=CENTER>\n",
    "\n",
    "# Introduction to NLP: Basic Concepts\n",
    "## Professor Robert J. Brunner\n",
    "  \n",
    "</DIV>  \n",
    "-----\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this IPython Notebook, we build on the [text analysis][w7i] concepts\n",
    "presented previously to dive more deeply into text analysis.\n",
    "Specifically, we will move beyond simple tokenization to leverage the\n",
    "semantic information contained in ordering and arrangement of text data\n",
    "to gain new insights. We will start by exploring alternative tokenization\n",
    "techniques provided by the NLTK library before delving into\n",
    "part-of-speech tagging and named entity recognition. \n",
    "\n",
    "We begin by parsing a simple text document that contains the course\n",
    "description for INFO 490 SP16 (i.e., this course). First, we will employ\n",
    "a sentence tokenizer, before changing to word, whitespace, and \n",
    "word/punctuation tokenizers.\n",
    "\n",
    "-----\n",
    "\n",
    "[w7i]: ../../Week7/index.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 sentances in course description\n",
      "----------------------------------------\n",
      "Students will first learn how to perform more statistical data exploration and constructing and evaluating statistical models.\n"
     ]
    }
   ],
   "source": [
    "# As a text example, we use the course description for INFO490  SP16.\n",
    "info_course = ['Advanced Data Science: This class is an asynchronous, online course.', \n",
    "               'This course will introduce advanced data science concepts by building on the foundational concepts presented in INFO 490: Foundations of Data Science.', \n",
    "               'Students will first learn how to perform more statistical data exploration and constructing and evaluating statistical models.', \n",
    "               'Next, students will learn machine learning techniques including supervised and unsupervised learning, dimensional reduction, and cluster finding.', \n",
    "               'An emphasis will be placed on the practical application of these techniques to high-dimensional numerical data, time series data, image data, and text data.', \n",
    "               'Finally, students will learn to use relational databases and cloud computing software components such as Hadoop, Spark, and NoSQL data stores.', \n",
    "               'Students must have access to a fairly modern computer, ideally that supports hardware virtualization, on which they can install software.', \n",
    "               'This class is open to sophomores, juniors, seniors and graduate students in any discipline who have either taken a previous INFO 490 data science course or have received instructor permission.']\n",
    "\n",
    "text = \" \".join(info_course)\n",
    "\n",
    "# Tokenize and display results. Also display one representative sentence\n",
    "from nltk import sent_tokenize\n",
    "snts = sent_tokenize(text)\n",
    "print('{0} sentances in course description'.format(len(snts)))\n",
    "print(40*'-')\n",
    "print(snts[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "185 words in course description\n",
      "----------------------------------------\n",
      "[ 'Advanced', 'Data', 'Science', ':', 'This', 'class', 'is', 'an',\n",
      "  'asynchronous', ',', 'online', 'course', '.']\n"
     ]
    }
   ],
   "source": [
    "# Tokenize by words, display results, and a representive section of words\n",
    "from nltk import word_tokenize\n",
    "wtks = word_tokenize(text)\n",
    "\n",
    "print('{0} words in course description'.format(len(wtks)))\n",
    "print(40*'-')\n",
    "\n",
    "# Display the tokens\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=2, depth=2, width=80, compact=True)\n",
    "\n",
    "pp.pprint(wtks[:13])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "161 words in course description (WS Tokenizer)\n",
      "----------------------------------------\n",
      "[ 'Advanced', 'Data', 'Science:', 'This', 'class', 'is', 'an', 'asynchronous,',\n",
      "  'online', 'course.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "tokenizer = WhitespaceTokenizer()\n",
    "wtks = tokenizer.tokenize(text)\n",
    "\n",
    "print('{0} words in course description (WS Tokenizer)'.format(len(wtks)))\n",
    "print(40*'-')\n",
    "\n",
    "pp.pprint(wtks[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "187 words in course description (WP Tokenizer)\n",
      "----------------------------------------\n",
      "[ 'Advanced', 'Data', 'Science', ':', 'This', 'class', 'is', 'an',\n",
      "  'asynchronous', ',', 'online', 'course', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer\n",
    "tokenizer = WordPunctTokenizer()\n",
    "wtks = tokenizer.tokenize(text)\n",
    "\n",
    "print('{0} words in course description (WP Tokenizer)'.format(len(wtks)))\n",
    "print(40*'-')\n",
    "\n",
    "pp.pprint(wtks[:13])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "### Collocations\n",
    "\n",
    "We previously discussed using multiple, adjacent words, which is known\n",
    "as n-grams (e.g., bigrams or trigrams). We can also build\n",
    "[collocations][nc], where we use NLTK to grab n-grams, but now with the\n",
    "possibility of applying filters, such as a minimum frequency of\n",
    "occurrence. We can employ an association measure, such as the [pointwise\n",
    "mutual information][wpmi] (PMI), to compute the importance of a\n",
    "collocation. PMI quantifies the likelihood of two words occurring together\n",
    "in a document to their chance superposition (from their individual\n",
    "distribution in the document). Thus, a PMI close to one implies two\n",
    "words almost always occur together, while a PMI close to zero implies\n",
    "two words are nearly independent and rarely occur together.\n",
    "\n",
    "-----\n",
    "[nc]: http://www.nltk.org/howto/collocations.html\n",
    "[wpmi]: https://en.wikipedia.org/wiki/Pointwise_mutual_information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best 10 bi-grams in course description (WP Tokenizer)\n",
      "--------------------------------------------------\n",
      "[ ('An', 'emphasis'),\n",
      "  ('an', 'asynchronous'),\n",
      "  ('any', 'discipline'),\n",
      "  ('as', 'Hadoop'),\n",
      "  ('be', 'placed'),\n",
      "  ('by', 'building'),\n",
      "  ('can', 'install'),\n",
      "  ('cloud', 'computing'),\n",
      "  ('cluster', 'finding'),\n",
      "  ('components', 'such')]\n",
      "--------------------------------------------------\n",
      "Best 10 bi-grams occuring more than once in course description (WP Tokenizer)\n",
      "--------------------------------------------------\n",
      "[ ('Data', 'Science'),\n",
      "  ('INFO', '490'),\n",
      "  ('class', 'is'),\n",
      "  ('This', 'class'),\n",
      "  ('on', 'the'),\n",
      "  ('students', 'will'),\n",
      "  ('will', 'learn'),\n",
      "  ('.', 'Students'),\n",
      "  ('data', 'science'),\n",
      "  ('.', 'This')]\n"
     ]
    }
   ],
   "source": [
    "top_bgs = 10\n",
    "\n",
    "from nltk.collocations import BigramCollocationFinder, BigramAssocMeasures\n",
    "bigram_measures = BigramAssocMeasures()\n",
    "finder = BigramCollocationFinder.from_words(wtks)\n",
    "bgs = finder.nbest(bigram_measures.pmi, top_bgs)\n",
    "\n",
    "print('Best {0} bi-grams in course description (WP Tokenizer)'.format(top_bgs))\n",
    "print(50*'-')\n",
    "\n",
    "ppf = pprint.PrettyPrinter(indent=2, depth=2, width=80, compact=False)\n",
    "ppf.pprint(bgs)\n",
    "\n",
    "print(50*'-')\n",
    "print('Best {0} bi-grams occuring more than once in course description (WP Tokenizer)'.format(top_bgs))\n",
    "print(50*'-')\n",
    "\n",
    "finder.apply_freq_filter(2)\n",
    "bgs = finder.nbest(bigram_measures.pmi, top_bgs)\n",
    "ppf.pprint(bgs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best 10 tri-grams in course description (WP Tokenizer)\n",
      "--------------------------------------------------\n",
      "[ ('any', 'discipline', 'who'),\n",
      "  ('components', 'such', 'as'),\n",
      "  ('fairly', 'modern', 'computer'),\n",
      "  ('ideally', 'that', 'supports'),\n",
      "  ('received', 'instructor', 'permission'),\n",
      "  ('such', 'as', 'Hadoop'),\n",
      "  ('supports', 'hardware', 'virtualization'),\n",
      "  ('that', 'supports', 'hardware'),\n",
      "  ('they', 'can', 'install'),\n",
      "  ('use', 'relational', 'databases')]\n",
      "--------------------------------------------------\n",
      "Best tri-grams occuring more than once in course description (WP Tokenizer)\n",
      "--------------------------------------------------\n",
      "[ ('This', 'class', 'is'),\n",
      "  ('students', 'will', 'learn'),\n",
      "  (',', 'students', 'will')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.collocations import TrigramAssocMeasures, TrigramCollocationFinder\n",
    "\n",
    "trigram_measures = TrigramAssocMeasures()\n",
    "finder = TrigramCollocationFinder.from_words(wtks)\n",
    "tgs = finder.nbest(trigram_measures.pmi, top_bgs)\n",
    "\n",
    "print('Best {0} tri-grams in course description (WP Tokenizer)'.format(top_bgs))\n",
    "print(50*'-')\n",
    "\n",
    "ppf = pprint.PrettyPrinter(indent=2, depth=2, width=80, compact=False)\n",
    "ppf.pprint(tgs)\n",
    "\n",
    "print(50*'-')\n",
    "print('Best tri-grams occuring more than once in course description (WP Tokenizer)')\n",
    "print(50*'-')\n",
    "\n",
    "finder.apply_freq_filter(2)\n",
    "tgs = finder.nbest(bigram_measures.pmi, top_bgs)\n",
    "ppf.pprint(tgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "## Tagging\n",
    "\n",
    "The simplest approach to text analysis is the bag-of-words model, where\n",
    "we simply identify the words (or tokens) present in a set of documents.\n",
    "In order to move beyond this model, we need to include additional\n",
    "information with each word. For example, the word _duck_ can mean the\n",
    "bird or it can mean the action. More generally, this concept when\n",
    "applied to multiple words is known as a [garden path sentences][wgps]. \n",
    "\n",
    "In the bag of word model, the difference between these two meanings (of\n",
    "the word _duck_) is lost. By associating information about the context\n",
    "or the grammatical nature of a word, however, these different use cases\n",
    "can be distinguished. The mechanism by which this is done is known as\n",
    "tagging. A tag can be used to identify the grammatical nature of a word,\n",
    "like _noun_ or _verb_, or it can be other information, including\n",
    "associations with other words in the text. In the following code blocks,\n",
    "we first introduce a _DefaultTagger_, which associates a tag of our\n",
    "choosing with words. Afterwards, we use the NLTK built-in Part of\n",
    "Speech (POS) and Named Entity Recognition (NER) taggers.\n",
    "\n",
    "-----\n",
    "[wgps]: https://en.wikipedia.org/wiki/Garden_path_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tagged course description (WP Tokenizer)\n",
      "--------------------------------------------------\n",
      "[ ('Advanced', 'INFO'), ('Data', 'INFO'), ('Science', 'INFO'), (':', 'INFO'),\n",
      "  ('This', 'INFO'), ('class', 'INFO'), ('is', 'INFO'), ('an', 'INFO'),\n",
      "  ('asynchronous', 'INFO'), (',', 'INFO'), ('online', 'INFO'),\n",
      "  ('course', 'INFO'), ('.', 'INFO')]\n"
     ]
    }
   ],
   "source": [
    "a_tag = 'INFO'\n",
    "\n",
    "from nltk.tag import DefaultTagger\n",
    "default_tagger = DefaultTagger(a_tag)\n",
    "tgs = default_tagger.tag(wtks)\n",
    "\n",
    "print('Tagged course description (WP Tokenizer)')\n",
    "print(50*'-')\n",
    "\n",
    "pp.pprint(tgs[:13])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "### Part of Speech Tagging\n",
    "\n",
    "Part of speech (PoS) simply refers to the grammatical properties of a word.\n",
    "While this might seem simple, given the diversity of languages (and even\n",
    "variations within a single language), this topic quickly becomes quite\n",
    "substantial. As a result, there are a number of possible approaches. In\n",
    "the next two code cells, we first demonstrate a simple PoS that labels\n",
    "only basic text components such as _Noun_, _Verb_, or _Adjective_,\n",
    "before moving to a more complex PoS that labels a wider range of text\n",
    "components, which can also establish grammatical relationships between\n",
    "multiple words.\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS tagged course description (WP Tokenizer/Univesal Tagger)\n",
      "------------------------------------------------------------\n",
      "[ ('Advanced', 'NOUN'),\n",
      "  ('Data', 'NOUN'),\n",
      "  ('Science', 'NOUN'),\n",
      "  (':', '.'),\n",
      "  ('This', 'DET'),\n",
      "  ('class', 'NOUN'),\n",
      "  ('is', 'VERB'),\n",
      "  ('an', 'DET'),\n",
      "  ('asynchronous', 'ADJ'),\n",
      "  (',', '.'),\n",
      "  ('online', 'ADJ'),\n",
      "  ('course', 'NOUN'),\n",
      "  ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "from nltk import pos_tag\n",
    "\n",
    "ptgs = pos_tag(wtks, tagset='universal')\n",
    "\n",
    "print('POS tagged course description (WP Tokenizer/Univesal Tagger)')\n",
    "print(60*'-')\n",
    "\n",
    "ppf.pprint(ptgs[:13])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "PoS tags can be much more complex, as shown in the following code cell.\n",
    "The specific tags depend on the selected tagset, by default NLTK now\n",
    "uses a [_PerceptronTagger_][pt], which quickly generates a set of tagged\n",
    "grammatical constructs.\n",
    "\n",
    "----\n",
    "[pt]: http://spacy.io/blog/part-of-speech-POS-tagger-in-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS tagged course description (WP Tokenizer/Default Tagger)\n",
      "------------------------------------------------------------\n",
      "[ ('Advanced', 'NNP'),\n",
      "  ('Data', 'NNP'),\n",
      "  ('Science', 'NN'),\n",
      "  (':', ':'),\n",
      "  ('This', 'DT'),\n",
      "  ('class', 'NN'),\n",
      "  ('is', 'VBZ'),\n",
      "  ('an', 'DT'),\n",
      "  ('asynchronous', 'JJ'),\n",
      "  (',', ','),\n",
      "  ('online', 'JJ'),\n",
      "  ('course', 'NN'),\n",
      "  ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "ptgs = pos_tag(wtks)\n",
    "\n",
    "print('POS tagged course description (WP Tokenizer/Default Tagger)')\n",
    "print(60*'-')\n",
    "\n",
    "ppf.pprint(ptgs[:13])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "### Named Entity Recognition\n",
    "\n",
    "Named Entity Recognition (NER) classifies (or recognizes) chunks of text\n",
    "that refer to pre-defined categories (or named entities). These chunks\n",
    "can be one or more words, and the categories can be names of people,\n",
    "organizations, locations, or other types of entities. For example, in\n",
    "the following sentence:\n",
    "\n",
    "> Edward is a graduate student enrolled at the University of Illinois.\n",
    "\n",
    "_Edward_ is a person and _University of Illinois_ is an organization.\n",
    "NLTK can be used to identify named entities, generally following a part\n",
    "of speech tagging (to clarify different uses of words that otherwise\n",
    "might cause confusion). In the following code cell, we demonstrate NER by\n",
    "using NLTK to identify named entities in the course description text.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "NER tagged course description (WP Tokenizer)\n",
      "--------------------------------------------------\n",
      "[ Tree('PERSON', [('Advanced', 'NNP')]),\n",
      "  Tree('ORGANIZATION', [('Data', 'NNP'), ('Science', 'NN')]),\n",
      "  (':', ':'),\n",
      "  ('This', 'DT'),\n",
      "  ('class', 'NN'),\n",
      "  ('is', 'VBZ'),\n",
      "  ('an', 'DT'),\n",
      "  ('asynchronous', 'JJ'),\n",
      "  (',', ','),\n",
      "  ('online', 'JJ'),\n",
      "  ('course', 'NN'),\n",
      "  ('.', '.'),\n",
      "  ('This', 'DT')]\n"
     ]
    }
   ],
   "source": [
    "from nltk import ne_chunk\n",
    "\n",
    "nrcs = ne_chunk(pos_tag(wtks))\n",
    "\n",
    "print(50*'-')\n",
    "print('NER tagged course description (WP Tokenizer)')\n",
    "print(50*'-')\n",
    "\n",
    "ppf.pprint(nrcs[:13])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "## Corpus\n",
    "\n",
    "A corpus is simply a collection of documents. In the case of Natural\n",
    "Language Processing, however, a corpus can include additional\n",
    "information for both part of speech tagging and named entity\n",
    "recognition. The NLTK library includes several corpuses, including the\n",
    "Penn Treebank, Brown, and Wordnet. In the rest of this notebook, we\n",
    "introduce the first two corpuses; the Wordnet corpus is introduced in\n",
    "[Introduction to NLP: Semantic Analysis][l3] notebook.\n",
    "\n",
    "###  Penn Treebank\n",
    "\n",
    "The [Penn Treebank project][ptbp] is an effort to annotate text, into a\n",
    "linguistic structure. This structure is generally in the form of a\n",
    "[tree][wt], within which the different components of a sentence are\n",
    "organized. This process includes a [part of speech tagging][ptpos]. We\n",
    "demonstrate the use of the Penn Treebank with NLTK in the next few code\n",
    "cells, where we tokenize text by using a Penn Treebank standard sentence\n",
    "and word tokenizer, and tagged sentence and word tokenizers. Finally, we\n",
    "introduce the `UnigramTagger`, which can be trained on a given corpus to\n",
    "tokenize and tag unigrams in a new document (or set of documents).\n",
    "\n",
    "-----\n",
    "[ptbp]: https://www.cis.upenn.edu/~treebank/\n",
    "[ptpos]: https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html\n",
    "[wt]: https://en.wikipedia.org/wiki/Treebank\n",
    "[l3]: intro2nlp-sa.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Penn Treebank tagged text.\n",
      "--------------------------------------------------------------------------------\n",
      "Words:     [ 'Pierre', 'Vinken', ',', '61', 'years', 'old', ',', 'will', 'join', 'the',\n",
      "  'board', 'as', 'a', 'nonexecutive', 'director', 'Nov.', '29', '.']\n",
      "--------------------------------------------------------------------------------\n",
      "Setnences: [ 'Pierre', 'Vinken', ',', '61', 'years', 'old', ',', 'will', 'join', 'the',\n",
      "  'board', 'as', 'a', 'nonexecutive', 'director', 'Nov.', '29', '.']\n",
      "--------------------------------------------------------------------------------\n",
      "Tagged Words: \n",
      "[ ('Pierre', 'NNP'), ('Vinken', 'NNP'), (',', ','), ('61', 'CD'),\n",
      "  ('years', 'NNS'), ('old', 'JJ'), (',', ','), ('will', 'MD'), ('join', 'VB'),\n",
      "  ('the', 'DT'), ('board', 'NN'), ('as', 'IN'), ('a', 'DT'),\n",
      "  ('nonexecutive', 'JJ'), ('director', 'NN'), ('Nov.', 'NNP'), ('29', 'CD'),\n",
      "  ('.', '.')]\n",
      "--------------------------------------------------------------------------------\n",
      "Tagged Sentances: \n",
      "[ ('Pierre', 'NNP'), ('Vinken', 'NNP'), (',', ','), ('61', 'CD'),\n",
      "  ('years', 'NNS'), ('old', 'JJ'), (',', ','), ('will', 'MD'), ('join', 'VB'),\n",
      "  ('the', 'DT'), ('board', 'NN'), ('as', 'IN'), ('a', 'DT'),\n",
      "  ('nonexecutive', 'JJ'), ('director', 'NN'), ('Nov.', 'NNP'), ('29', 'CD'),\n",
      "  ('.', '.')]\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import treebank\n",
    "\n",
    "print('Penn Treebank tagged text.')\n",
    "print(80*'-')\n",
    "\n",
    "print('Words:     ', end='')\n",
    "pp.pprint(treebank.words()[:18])\n",
    "print(80*'-')\n",
    "\n",
    "print('Setnences: ', end='')\n",
    "pp.pprint(treebank.sents()[0])\n",
    "print(80*'-')\n",
    "\n",
    "print('Tagged Words: ')\n",
    "pp.pprint(treebank.tagged_words()[:18])\n",
    "print(80*'-')\n",
    "\n",
    "print('Tagged Sentances: ')\n",
    "pp.pprint(treebank.tagged_sents()[0])\n",
    "print(80*'-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.tag import UnigramTagger\n",
    "pt_tagger = UnigramTagger(treebank.tagged_sents())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Penn Treebank tagged course description (WP Tokenizer)\n",
      "------------------------------------------------------------\n",
      "[ ('Advanced', 'NNP'),\n",
      "  ('Data', 'NNP'),\n",
      "  ('Science', 'NN'),\n",
      "  (':', ':'),\n",
      "  ('This', 'DT'),\n",
      "  ('class', 'NN'),\n",
      "  ('is', 'VBZ'),\n",
      "  ('an', 'DT'),\n",
      "  ('asynchronous', None),\n",
      "  (',', ','),\n",
      "  ('online', None),\n",
      "  ('course', 'NN'),\n",
      "  ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "pt_tgs = pt_tagger.tag(wtks)\n",
    "\n",
    "print('Penn Treebank tagged course description (WP Tokenizer)')\n",
    "print(60*'-')\n",
    "\n",
    "ppf.pprint(pt_tgs[:13])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "### Brown Corpus\n",
    "\n",
    "The [Brown Corpus][wbc] has over one million tagged words, and was\n",
    "originally published in 1967. The corpus itself is composed of 500\n",
    "samples, spread over fifteen different genres, of English-language text\n",
    "compiled from works published in 1961. NLTK provides the Brown Corpus,\n",
    "which can be used to tag new documents, as shown below.\n",
    "\n",
    "----\n",
    "[wbc]: https://en.wikipedia.org/wiki/Brown_Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import brown\n",
    "\n",
    "b_tagger = UnigramTagger(brown.tagged_sents(brown.fileids()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Brown tagged course description (WP Tokenizer)\n",
      "------------------------------------------------------------\n",
      "[ ('Advanced', 'JJ-TL'),\n",
      "  ('Data', 'NNS-TL'),\n",
      "  ('Science', 'NN-TL'),\n",
      "  (':', ':'),\n",
      "  ('This', 'DT'),\n",
      "  ('class', 'NN'),\n",
      "  ('is', 'BEZ'),\n",
      "  ('an', 'AT'),\n",
      "  ('asynchronous', None),\n",
      "  (',', ','),\n",
      "  ('online', None),\n",
      "  ('course', 'NN'),\n",
      "  ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "b_tgs = b_tagger.tag(wtks)\n",
    "\n",
    "print('Brown tagged course description (WP Tokenizer)')\n",
    "print(60*'-')\n",
    "\n",
    "ppf.pprint(b_tgs[:13])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "### Linking Taggers\n",
    "\n",
    "\n",
    "In the previous examples, certain words were left untagged or tagged\n",
    "with `None` (such as _online_ or _asynchronous_). Since language evolves\n",
    "over time, an older corpus might miss words, or they may simply be\n",
    "incomplete. To handle these cases, NLTK enables taggers to be linked.\n",
    "Thus a general tagger can be applied, such as the Brown Corpus,\n",
    "after which a second tagger can be applied to increase the number of\n",
    "words tagged. This is a common application area for a _DefaultTagger_,\n",
    "which can be used to assign a specific tag to any element missed by\n",
    "another tagger. We demonstrate this concept below, by linking the Brown\n",
    "Corpus tagger with our earlier Default tagger.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Brown tagged course description (WP Tokenizer/Linked Tagger)\n",
      "------------------------------------------------------------\n",
      "[ ('Advanced', 'JJ-TL'),\n",
      "  ('Data', 'NNS-TL'),\n",
      "  ('Science', 'NN-TL'),\n",
      "  (':', ':'),\n",
      "  ('This', 'DT'),\n",
      "  ('class', 'NN'),\n",
      "  ('is', 'BEZ'),\n",
      "  ('an', 'AT'),\n",
      "  ('asynchronous', 'INFO'),\n",
      "  (',', ','),\n",
      "  ('online', 'INFO'),\n",
      "  ('course', 'NN'),\n",
      "  ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "# We can link taggers\n",
    "\n",
    "b_tagger._taggers = [b_tagger, default_tagger]\n",
    "\n",
    "b_tgs = b_tagger.tag(wtks)\n",
    "\n",
    "print('Brown tagged course description (WP Tokenizer/Linked Tagger)')\n",
    "print(60*'-')\n",
    "\n",
    "ppf.pprint(b_tgs[:13])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "### Tagged Text Extraction\n",
    "\n",
    "For some text analysis projects, we might want to restrict words (or\n",
    "tokens) to specific tags. For example, we might prefer to only use\n",
    "_Nouns_, _Primary Verbs_, or _Adjectives_ for text classification. To\n",
    "extract only terms that meet these conditions, we can tag the text, and\n",
    "apply a regular expression to the tagged tokens, as shown in the\n",
    "following code cell.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS tagged course description (WP Tokenizer)\n",
      "------------------------------------------------------------\n",
      "[ ('Advanced', 'NNP'), ('Data', 'NNP'), ('Science', 'NN'), (':', ':'),\n",
      "  ('This', 'DT'), ('class', 'NN'), ('is', 'VBZ'), ('an', 'DT'),\n",
      "  ('asynchronous', 'JJ'), (',', ','), ('online', 'JJ'), ('course', 'NN'),\n",
      "  ('.', '.')]\n",
      "------------------------------------------------------------\n",
      "POS tagged course description (WP Tokenizer/RegEx applied)\n",
      "------------------------------------------------------------\n",
      "['Advanced', 'Data', 'Science', 'class', 'asynchronous', 'online', 'course']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# NN matchs NN|NNS|NNP|NNPS\n",
    "rgxs = re.compile(r\"(JJ|NN|VBN|VBG)\")\n",
    "\n",
    "ptgs = pos_tag(wtks)\n",
    "trms = [tkn[0] for tkn in ptgs if re.match(rgxs, tkn[1])]\n",
    "\n",
    "print('POS tagged course description (WP Tokenizer)')\n",
    "print(60*'-')\n",
    "pp.pprint(ptgs[:13])\n",
    "print(60*'-')\n",
    "print('POS tagged course description (WP Tokenizer/RegEx applied)')\n",
    "print(60*'-')\n",
    "pp.pprint(trms[:7])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "### Student Activity\n",
    "\n",
    "In the preceding cells, we introduced several basic NLP concepts,\n",
    "including tagging, Part of Speech, and Named Entity Recognition. Now\n",
    "that you have run the Notebook, go back and make the following changes\n",
    "to see how the results change.\n",
    "\n",
    "1. Change from a Unigram tagger to a Bigram Tagger. How do you results\n",
    "change?\n",
    "2. Replace the initial text with a longer document (you can use a text\n",
    "from within NLTK or a freely available text from _Project Gutenberg_).\n",
    "Apply more restrictive filters (i.e., higher frequencies) to the bigrams\n",
    "and trigrams, do your results make sense?\n",
    "3. Try using regular expressions to restrict tokens in the NLTK movie\n",
    "review data set to Nouns, Verbs, Adjectives, and Adverbs. Use these\n",
    "tokens to perform Sentiment Analysis on these movie review data. Are the\n",
    "results better or worse than with all words?\n",
    "\n",
    "-----"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
