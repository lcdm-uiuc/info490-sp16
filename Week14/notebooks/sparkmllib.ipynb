{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<DIV ALIGN=CENTER>\n",
    "\n",
    "# Introduction to Spark: Machine Learning\n",
    "## Professor Robert J. Brunner\n",
    "  \n",
    "</DIV>  \n",
    "-----\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this IPython Notebook, we explore using Spark to perform basic\n",
    "statistical analysis and machine learning. For part of this analysis, we\n",
    "will use the airline data, which has been stored in files that are\n",
    "accessible from within our Spark cluster. The [current version of\n",
    "Spark][sv] has two machine learning packages. The original, and best\n",
    "developed machine learning library is [MLlib][mll], while the newer\n",
    "library is [ML][ml]. The former operates on Spark RDDs, while the latter\n",
    "operates on DataFrames, which seem to be the future of Spark data\n",
    "structures. Given the current dominance of MLlib, we focus on the\n",
    "original Spark machine learning package in this Notebook.\n",
    "\n",
    "-----\n",
    "\n",
    "[sv]: https://spark.apache.org\n",
    "[mll]: https://spark.apache.org/mllib/\n",
    "[ml]: https://spark.apache.org/docs/latest/ml-guide.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization\n",
    "\n",
    "In this class, we have a dedicated Spark cluster running to allow\n",
    "students to explore Spark from within our IPython Notebook environment.\n",
    "Since our Spark cluster has limited resources, we need to carefully\n",
    "manage them, in particular we need to ensure that any SparkContext\n",
    "previously used by this Jupyter Server is properly released before\n",
    "starting a new one. After this, we will initialize a new SparkContext to\n",
    "properly interact from this dockerized IPython Notebook to the Spark\n",
    "cluster.\n",
    "\n",
    "----- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Spark version: 1.6.0\n"
     ]
    }
   ],
   "source": [
    "# We release the SparkContext if it exists.\n",
    "try:\n",
    "    sc\n",
    "except:\n",
    "    pass ;\n",
    "else:\n",
    "    sc.stop()\n",
    "\n",
    "# Now handle initial import statements\n",
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "# Create new Spark Configuration (port numbers might need to be adjusted from defaults.)\n",
    "myconf = SparkConf()\n",
    "myconf.setMaster('local[*]')\n",
    "myconf.setAppName(\"INFO490 SP16 W14-NB3: Professor Brunner\")\n",
    "myconf.set('spark.executor.memory', '1g')\n",
    "\n",
    "# Create and initialize a new Spark Context\n",
    "sc = SparkContext(conf=myconf)\n",
    "\n",
    "# Display Spark version information, which also verifies SparkContext is active\n",
    "print(\"\\nSpark version: {0}\".format(sc.version))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "### Data Processing\n",
    "\n",
    "In this Notebook, we will need sample data. To simplify acquiring data\n",
    "to demonstrate using Spark DataFrames, we include the RDD code from the\n",
    "[Introduction to Spark](intro2spark.ipynb) Notebook in the following\n",
    "cell.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entries in fields RDD = 480106\n"
     ]
    }
   ],
   "source": [
    "filename = '/home/data_scientist/data/2001/2001-1.csv'\n",
    "\n",
    "text_file = sc.textFile(filename)\n",
    "\n",
    "col_data = text_file.map(lambda l: l.split(\",\")) \\\n",
    "            .map(lambda p: (p[0], p[1], p[2], p[4], p[14], p[15], p[16], p[17], p[18])) \\\n",
    "            .filter(lambda line: 'Year' not in line)\n",
    "\n",
    "cols = col_data.filter(lambda line: 'NA' not in line)\n",
    "\n",
    "fields = cols.map(lambda p: (int(p[0]), int(p[1]), int(p[2]), int(p[3]),\n",
    "                          int(p[4]), int(p[5]), p[6], p[7], int(p[8])))\n",
    "\n",
    "# Should be 480106 if everything works correctly\n",
    "print('Number of entries in fields RDD = {0}'.format(fields.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "## Spark Statistics\n",
    "\n",
    "The simplest type of data analysis is to compute basic statistical\n",
    "measures of sequences of data. The Spark MLlib package includes a \n",
    "[basic statistical][sbs] component that can be easily used to obtain\n",
    "statistical measurements of multiple columns in a Spark RDD. We\n",
    "demonstrate this in the following code cells, where we create an RDD\n",
    "from numeric columns in our `fields` RDD. We use the `colStats` function\n",
    "from the `Statistics` object to compute a range of statistical measures\n",
    "in one pass for all columns in the `sdt` RDD. In the second code cell,\n",
    "we simply provide a nicely formatted display of these quantities for\n",
    "each column.\n",
    "\n",
    "-----\n",
    "\n",
    "[sbs]: https://spark.apache.org/docs/latest/mllib-statistics.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.stat import Statistics\n",
    "\n",
    "# Extract numeric columns and compute statistics\n",
    "sdt = fields.map(lambda p: (p[2], p[3], p[4], p[5], p[8]))\n",
    "summary = Statistics.colStats(sdt)\n",
    "\n",
    "# Extract individual statistics for RDD\n",
    "mus = summary.mean()\n",
    "mns = summary.min()\n",
    "mxs = summary.max()\n",
    "vrs = summary.variance()\n",
    "nnzs = summary.numNonzeros()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                Mean    Variance     Min       Max  Non Zeroes\n",
      "-----------------------------------------------------------------\n",
      "Day            16.01       79.87    1.00     31.00      480106\n",
      "Dep. Time    1359.66   237399.85    1.00   2400.00      480106\n",
      "Arr. Delay      6.38      964.02  -80.00   1688.00      461157\n",
      "Dep. Delay      8.78      782.11  -59.00   1692.00      393503\n",
      "Distance      716.99   323369.33   21.00   4962.00      480106\n"
     ]
    }
   ],
   "source": [
    "# Labels for display\n",
    "cols = ['Day', 'Dep. Time', 'Arr. Delay', 'Dep. Delay', 'Distance']\n",
    "\n",
    "# Print out Header\n",
    "print('{0:>20s}{1:>12s}{2:>8s}{3:>10s}{4:>12s}'\\\n",
    "      .format('Mean', 'Variance', 'Min', 'Max', 'Non Zeroes'))\n",
    "print(65*'-')\n",
    "\n",
    "# Printout summary statistics\n",
    "for idx, (m, v, mn, mx, n) in enumerate(zip(mus, vrs, mns, mxs, nnzs)):\n",
    "    print('{5:10s}{0:10.2f}{1:12.2f}{2:8.2f}{3:10.2f}{4:12d}'\\\n",
    "          .format(m, v, mn, mx, int(n), cols[idx]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "### Correlations\n",
    "\n",
    "Another useful function is to compute the correlation between different\n",
    "data sequences. The Spark MLlib package includes the `corr` method\n",
    "within the Statistics component to compute correlations between\n",
    "individual data sequences, or via the columns in an RDD. The `corr`\n",
    "method can also calculate either the _Pearson_ correlation, which is the\n",
    "default, or the _Spearman_ correlation. In the first code cell, we\n",
    "create several data sequences, turn them into Spark data structures via\n",
    "the `parallelize` method, and compute the Pearson correlation\n",
    "coefficient between the different data sequences. In the second code\n",
    "cell, we create a new RDD from three columns in the `sdt` RDD, and\n",
    "compute both the Pearson and Spearman correlations between the columns\n",
    "in this RDD.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x =  [0, 1, 2]\n",
      "y =  [1, 2, 4]\n",
      "z =  [2, 1, 0]\n",
      "\n",
      "Pearson Correlation Tests\n",
      "-------------------------\n",
      "x corr x = +1.000\n",
      "x corr y = +0.982\n",
      "x corr z = -1.000\n"
     ]
    }
   ],
   "source": [
    "# Demonstrate Correlation Measurements\n",
    "\n",
    "# Sample Data\n",
    "x = sc.parallelize([0, 1, 2])\n",
    "y = sc.parallelize([1, 2, 4])\n",
    "z = sc.parallelize([2, 1, 0])\n",
    "\n",
    "print('x = ', x.collect())\n",
    "print('y = ', y.collect())\n",
    "print('z = ', z.collect())\n",
    "\n",
    "print('\\nPearson Correlation Tests')\n",
    "print(25*'-')\n",
    "print('x corr x = {0:+5.3f}'\\\n",
    "      .format(Statistics.corr(x, x, method='pearson')))\n",
    "\n",
    "print('x corr y = {0:+5.3f}'\\\n",
    "      .format(Statistics.corr(x, y, method='pearson')))\n",
    "\n",
    "print('x corr z = {0:+5.3f}'\\\n",
    "      .format(Statistics.corr(x, z, method='pearson')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dearture Time, Arrival Delay, Departure Delay\n",
      "\n",
      "Pearson Correlation Matrix:\n",
      "[[ 1.     0.134  0.167]\n",
      " [ 0.134  1.     0.904]\n",
      " [ 0.167  0.904  1.   ]]\n",
      "\n",
      "Spearman Correlation Matrix:\n",
      "[[ 1.     0.109  0.173]\n",
      " [ 0.109  1.     0.616]\n",
      " [ 0.173  0.616  1.   ]]\n"
     ]
    }
   ],
   "source": [
    "# Set print precision of matrices\n",
    "import numpy as np\n",
    "np.set_printoptions(precision=3)\n",
    "\n",
    "# Compute correlation of three columns in RDD\n",
    "cd = sdt.map(lambda p: (p[1], p[2], p[3]))\n",
    "\n",
    "print('Dearture Time, Arrival Delay, Departure Delay')\n",
    "\n",
    "print('\\nPearson Correlation Matrix:')\n",
    "print(Statistics.corr(cd, method='pearson'))\n",
    "\n",
    "print('\\nSpearman Correlation Matrix:')\n",
    "print(Statistics.corr(cd, method='spearman'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "### Random Data and Sampling\n",
    "\n",
    "Another useful capability when constructing models is to generate random\n",
    "data from a particular theoretical statistical distribution, such as a\n",
    "_Normal_, _Uniform_, or _Poisson_ distribution. Likewise, when building\n",
    "a model from large data, one often needs to sample from the large data\n",
    "to make a more manageable data set with which to construct a model. The\n",
    "Spark MLlib package provides methods for both of these features.\n",
    "\n",
    "First, the `RandomRDDs` class includes methods to generate RDDs of a\n",
    "given size from a particular distribution, which is specified in the\n",
    "method called. In the following code cell, we create a distribution\n",
    "containing 1000 rows from a uniform, normal, and Poisson distribution.\n",
    "Afterwards, we compute several basic statistical measures from these\n",
    "distributions to demonstrate the simplicity of this approach to generate\n",
    "random data from model distributions.\n",
    "\n",
    "Second, we sample from the normal distribution both with and without\n",
    "replacement to make new samples. Afterwards, we once again compute basic\n",
    "statistical measures to demonstrate the random sampling within Spark\n",
    "MLlib.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.random import RandomRDDs\n",
    "\n",
    "ud = RandomRDDs.uniformRDD(sc, 1000, seed=23)\n",
    "\n",
    "nd = RandomRDDs.normalRDD(sc, 1000, seed=23)\n",
    "\n",
    "pd = RandomRDDs.poissonRDD(sc, mean=2.0, size=1000, seed=23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uniform Distribution Statistics\n",
      " (count: 1000, mean: 0.495907509202282, stdev: 0.298581265498, max: 0.99957542053, min: 0.000220626980565)\n"
     ]
    }
   ],
   "source": [
    "print('Uniform Distribution Statistics\\n', ud.stats())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal Distribution Statistics\n",
      " (count: 1000, mean: -0.01951879687296531, stdev: 0.936332160006, max: 2.76048478382, min: -3.10768336984)\n"
     ]
    }
   ],
   "source": [
    "print('Normal Distribution Statistics\\n', nd.stats())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Poisson Distribution Statistics\n",
      " (count: 1000, mean: 2.0089999999999995, stdev: 1.45771019068, max: 9.0, min: 0.0)\n"
     ]
    }
   ],
   "source": [
    "print('Poisson Distribution Statistics\\n', pd.stats())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(count: 239, mean: -0.08936987870555395, stdev: 0.948180629754, max: 2.70606620358, min: -2.93055151581)\n"
     ]
    }
   ],
   "source": [
    "# Sample without replacement\n",
    "\n",
    "frac = 0.25\n",
    "\n",
    "ds = nd.sample(False, frac)\n",
    "print(ds.stats())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(count: 262, mean: -0.0075864220820856965, stdev: 0.83686194341, max: 2.64550887934, min: -2.38312734807)\n"
     ]
    }
   ],
   "source": [
    "# Sample with replacement\n",
    "ds = nd.sample(True, frac)\n",
    "print(ds.stats())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "## Machine Learning\n",
    "\n",
    "The bulk of the MLlib package is focused on performing machine learning\n",
    "at scale by using Spark. With functions for computing classification,\n",
    "regression, clustering, dimensional reduction, and more, the library\n",
    "extends considerable power to the Spark user. Since we have already\n",
    "covered these concepts by using Python and scikit-learn, in the rest of\n",
    "this Notebook, we will present two specific machine learning algorithms\n",
    "in order to demonstrate the basic concepts required to work with the\n",
    "tools in the Spark MLlib package.\n",
    "\n",
    "-----\n",
    "\n",
    "### Linear Modeling\n",
    "\n",
    "One of the simplest machine learning techniques is [linear regression][slr].\n",
    "The main difference when using Spark is that for this supervised\n",
    "learning technique our data must be in a Spark specific data structure\n",
    "called [`LabeledPoint`][slp]. Spark provides several data structures to\n",
    "simplify the application of distributed machine learning algorithms at\n",
    "scale. The labeled nature refers to the label, used for training, that\n",
    "is associated with the point. The first item in the data structure is\n",
    "the label, while the second item is the set of feature columns.\n",
    "\n",
    "In the following code cells, we first create a new data structure that\n",
    "extracts the arrival delay to be the label and the departure delay as\n",
    "the feature. These data re turned into a Spark sequence containing\n",
    "`LabeledPoint` values for each row in the original RDD. Next we display\n",
    "the first rows in the new sequence, and next we train the linear\n",
    "regressor (using SVD in this case) and specify a number of iterations\n",
    "and step size. You should feel free to modify these values and see the\n",
    "impact on the resulting performance. Finally, we compute several\n",
    "regression metrics to quantify the performance of this method on these\n",
    "data (recall that the data span a large range, hence the RMSE is quite\n",
    "reasonable).\n",
    "\n",
    "-----\n",
    "\n",
    "[slp]: https://spark.apache.org/docs/latest/mllib-data-types.html#labeled-point\n",
    "[slr]: https://spark.apache.org/docs/latest/mllib-linear-methods.html#linear-least-squares-lasso-and-ridge-regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.regression import LinearRegressionWithSGD\n",
    "from pyspark.mllib.regression import LinearRegressionModel\n",
    "\n",
    "# Minimum departure delay\n",
    "min_delay = 5.\n",
    "data = fields.filter(lambda p: p[5] > min_delay).map(lambda p: LabeledPoint(p[4], [p[5]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LabeledPoint(23.0, [11.0]),\n",
       " LabeledPoint(18.0, [20.0]),\n",
       " LabeledPoint(96.0, [100.0]),\n",
       " LabeledPoint(20.0, [17.0]),\n",
       " LabeledPoint(87.0, [97.0])]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ln_model = LinearRegressionWithSGD.train(data, intercept=False, iterations=100, step=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vnp = data.map(lambda lp: (lp.label, float(ln_model.predict(lp.features))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(23.0, 10.730386646572505),\n",
       " (18.0, 19.5097939028591),\n",
       " (96.0, 97.54896951429551),\n",
       " (20.0, 16.583324817430235),\n",
       " (87.0, 94.62250042886664)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vnp.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE =  15.4\n",
      "MSE = 238.1\n",
      "MAE =  10.8\n",
      "r2 =   0.9\n",
      "EV = 2014.8\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.evaluation import RegressionMetrics\n",
    "\n",
    "tm = RegressionMetrics(vnp)\n",
    "\n",
    "print('RMSE = {0:5.1f}'.format(tm.rootMeanSquaredError))\n",
    "print('MSE = {0:5.1f}'.format(tm.meanSquaredError))\n",
    "print('MAE = {0:5.1f}'.format(tm.meanAbsoluteError))\n",
    "print('r2 = {0:5.1f}'.format(tm.r2))\n",
    "print('EV = {0:5.1f}'.format(tm.explainedVariance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(weights=[0.975489695143], intercept=0.0)\n"
     ]
    }
   ],
   "source": [
    "print(ln_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "### Random Forest\n",
    "\n",
    "The second machine learning algorithm we demonstrate is \n",
    "[Random Forests][srf], an ensemble, supervised learning technique. Once again,\n",
    "we need a data sequence of `LabeledPoint`, but in this case we simply\n",
    "reuse the one we created for the linear regression example. The next\n",
    "step is to apply a `trainregressor` on our random forest object. The\n",
    "random forest can accept categorical data, but in this case none of our\n",
    "columns are categorical and we specify this with an empty set. Finally,\n",
    "we explicitly set the number of trees in this example to one, which\n",
    "allows us to easily display the generated forest in the second code\n",
    "cell. \n",
    "\n",
    "Next, we predict values for our data. Technically, we would want to use\n",
    "a test-train split or even cross-validation to properly evaluate our\n",
    "model, but for simplicity we simply demonstrate the prediction and\n",
    "quality assessment on the entire training data. In order to compute the\n",
    "regression metrics from the random forest, we need to employ a slightly\n",
    "different strategy top combine the labels with the predictions, which is\n",
    "shown in the third code cell. Finally, we display the regression metrics\n",
    "for the random forest regression on the flight data.\n",
    "\n",
    "-----\n",
    "\n",
    "[srf]: https://spark.apache.org/docs/latest/mllib-ensembles.html#random-forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.tree import RandomForest\n",
    "\n",
    "rf_model = RandomForest.trainRegressor(data, categoricalFeaturesInfo={}, numTrees=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TreeEnsembleModel regressor with 1 trees\n",
      "\n",
      "  Tree 0:\n",
      "    If (feature 0 <= 73.0)\n",
      "     If (feature 0 <= 30.0)\n",
      "      If (feature 0 <= 16.0)\n",
      "       If (feature 0 <= 11.0)\n",
      "        Predict: 5.726113904806455\n",
      "       Else (feature 0 > 11.0)\n",
      "        Predict: 11.548860895202358\n",
      "      Else (feature 0 > 16.0)\n",
      "       If (feature 0 <= 23.0)\n",
      "        Predict: 17.536144237834172\n",
      "       Else (feature 0 > 23.0)\n",
      "        Predict: 24.667233253496097\n",
      "     Else (feature 0 > 30.0)\n",
      "      If (feature 0 <= 50.0)\n",
      "       If (feature 0 <= 41.0)\n",
      "        Predict: 33.07445158431199\n",
      "       Else (feature 0 > 41.0)\n",
      "        Predict: 43.313392081419885\n",
      "      Else (feature 0 > 50.0)\n",
      "       If (feature 0 <= 63.0)\n",
      "        Predict: 53.75277175990824\n",
      "       Else (feature 0 > 63.0)\n",
      "        Predict: 65.57393428063943\n",
      "    Else (feature 0 > 73.0)\n",
      "     If (feature 0 <= 134.0)\n",
      "      If (feature 0 <= 101.0)\n",
      "       If (feature 0 <= 85.0)\n",
      "        Predict: 77.0085324232082\n",
      "       Else (feature 0 > 85.0)\n",
      "        Predict: 91.61710137133637\n",
      "      Else (feature 0 > 101.0)\n",
      "       Predict: 114.76513545347467\n",
      "     Else (feature 0 > 134.0)\n",
      "      Predict: 193.67233835683032\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(rf_model.toDebugString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pr = rf_model.predict(data.map(lambda x: x.features))\n",
    "pnl = data.map(lambda lp: lp.label).zip(pr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE =  22.1\n",
      "MSE = 490.2\n",
      "MAE =  12.0\n",
      "r2 =   0.7\n",
      "EV = 2012.6\n"
     ]
    }
   ],
   "source": [
    "tm = RegressionMetrics(pnl)\n",
    "\n",
    "print('RMSE = {0:5.1f}'.format(tm.rootMeanSquaredError))\n",
    "print('MSE = {0:5.1f}'.format(tm.meanSquaredError))\n",
    "print('MAE = {0:5.1f}'.format(tm.meanAbsoluteError))\n",
    "print('r2 = {0:5.1f}'.format(tm.r2))\n",
    "print('EV = {0:5.1f}'.format(tm.explainedVariance))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "### Student Activity\n",
    "\n",
    "In the preceding cells, we introduced basic statistical analysis and\n",
    "machine learning with Spark. Now that you have run the Notebook, go back\n",
    "and make the following changes to see how the results change.\n",
    "\n",
    "1. Compute the Pearson and Spearman correlations between the departure\n",
    "and arrival delays in the flight data.\n",
    "\n",
    "2. Add an intercept value into the Linear regression, how does the slope\n",
    "of the new fit differs from the original fit in this Notebook.\n",
    "\n",
    "3. Add more columns into the Linear Regression demonstrated in this\n",
    "Notebook. In particular, include departure time and distance into the\n",
    "calculation.\n",
    "\n",
    "4. Build a Random Forest regressor with more tree. Which gives the best\n",
    "RMSE, 1 tree, 5 trees, 10 trees, or 25 trees? Can you explain why?\n",
    "\n",
    "4. In Week 2, we performed a logistic regression on the flight data to\n",
    "determine whether a flight would be late or not. Repeat this analysis,\n",
    "but use the Logistic Regression functionality within Spark MLlib."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "### Ending the Spark Session\n",
    "\n",
    "We must stop the `SparkContext` in order to release resources on the\n",
    "instructional cluster before existing this Notebook.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
