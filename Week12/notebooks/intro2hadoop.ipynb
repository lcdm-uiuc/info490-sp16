{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<DIV ALIGN=CENTER>\n",
    "\n",
    "# Introduction to Hadoop\n",
    "## Professor Robert J. Brunner\n",
    "  \n",
    "</DIV>  \n",
    "----- \n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this Notebook, we will introduce Hadoop and the Hadoop Distributed\n",
    "File System, which underlie the entire Hadoop ecosystem. Our setup will\n",
    "be using a single Hadoop node, which will not be very fast, especially\n",
    "when compared to simply running Python code directly. However, even with\n",
    "this simple setup, the full Hadoop process will be demonstrated,\n",
    "including the use of the Hadoop file system (HDFS) and the Hadoop\n",
    "Streaming process model.\n",
    "\n",
    "Typically, Hadoop is operated on a large cluster that runs both Hadoop\n",
    "and HDFS, although with the development of Yarn, more diverse workflows\n",
    "are now possible. In this Notebook, we only explore the basic Hadoop\n",
    "components of Hadoop and HDFS, which work together to run code on the\n",
    "nodes that hold the relevant data in order to maximize throughput.\n",
    "[Other resources][hort] exist to learn more about Yarn and other Hadoop\n",
    "workflows, and we will explore using [Pig][nb3] this week, and using\n",
    "Spark in Week 14.\n",
    "\n",
    "In the first code cell, we simple test the Hadoop is working in our\n",
    "course [Docker container][info-hadoop]. We do this by displaying the\n",
    "contents of the most recent Hadoop log files. If the files don't exist,\n",
    "or do not have current timestamps, you will need to explore whether\n",
    "Hadoop is working correctly before proceeding through the rest of this\n",
    "Notebook.\n",
    "\n",
    "-----\n",
    "[hort]: http://hortonworks.com/hadoop-tutorial/introducing-apache-hadoop-developers/\n",
    "[wcp]: https://hadoop.apache.org/docs/r2.6.0/hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapReduceTutorial.html\n",
    "\n",
    "[nb3]: intro2pig.ipynb\n",
    "\n",
    "[info-hadoop]: https://github.com/UI-DataScience/docker-info490/tree/master/hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### Out File #####\n",
      "ulimit -a for user data_scientist\n",
      "core file size          (blocks, -c) 0\n",
      "data seg size           (kbytes, -d) unlimited\n",
      "scheduling priority             (-e) 0\n",
      "file size               (blocks, -f) unlimited\n",
      "pending signals                 (-i) 7902\n",
      "max locked memory       (kbytes, -l) 64\n",
      "max memory size         (kbytes, -m) unlimited\n",
      "open files                      (-n) 1048576\n",
      "pipe size            (512 bytes, -p) 8\n",
      "POSIX message queues     (bytes, -q) 819200\n",
      "real-time priority              (-r) 0\n",
      "stack size              (kbytes, -s) 8192\n",
      "cpu time               (seconds, -t) unlimited\n",
      "max user processes              (-u) 1048576\n",
      "virtual memory          (kbytes, -v) unlimited\n",
      "file locks                      (-x) unlimited\n",
      "\n",
      "##### Log File #####\n",
      "2016-04-09 18:55:02,691 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 1ms\n",
      "2016-04-09 18:55:02,976 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/tmp/hadoop-data_scientist/dfs/data, DS-b80bc868-2c9c-4a7b-aa9e-b4729d030a22): no suitable block pools found to scan.  Waiting 1517975536 ms.\n",
      "2016-04-09 18:55:02,980 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1460243619980 with interval 21600000\n",
      "2016-04-09 18:55:02,982 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-629745585-172.17.0.3-1459931584833 (Datanode Uuid null) service to a01f005a25fc/172.17.0.1:9000 beginning handshake with NN\n",
      "2016-04-09 18:55:03,013 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-629745585-172.17.0.3-1459931584833 (Datanode Uuid null) service to a01f005a25fc/172.17.0.1:9000 successfully registered with NN\n",
      "2016-04-09 18:55:03,013 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode a01f005a25fc/172.17.0.1:9000 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000\n",
      "2016-04-09 18:55:03,101 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-629745585-172.17.0.3-1459931584833 (Datanode Uuid 5df2a193-dec4-4f93-82b4-3262cda64f72) service to a01f005a25fc/172.17.0.1:9000 trying to claim ACTIVE state with txid=4\n",
      "2016-04-09 18:55:03,101 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-629745585-172.17.0.3-1459931584833 (Datanode Uuid 5df2a193-dec4-4f93-82b4-3262cda64f72) service to a01f005a25fc/172.17.0.1:9000\n",
      "2016-04-09 18:55:03,165 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0xd0e55c4dd,  containing 1 storage report(s), of which we sent 1. The reports had 0 total blocks and used 1 RPC(s). This took 3 msec to generate and 62 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.\n",
      "2016-04-09 18:55:03,165 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-629745585-172.17.0.3-1459931584833\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "#!/usr/bin/env bash\n",
    "\n",
    "echo '##### Out File #####'\n",
    "out_file=$(ls -la /usr/local/hadoop/logs/hadoop-data*.out | head -1 | awk '{print $9}')\n",
    "cat  $out_file\n",
    "\n",
    "echo\n",
    "echo '##### Log File #####'\n",
    "log_file=$(ls -la /usr/local/hadoop/logs/hadoop-data*.log | head -1 | awk '{print $9}')\n",
    "tail -10  $log_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "## HDFS\n",
    "\n",
    "Next, we need to move our data to process into the Hadoop Distributed\n",
    "File system, or HDFS. HDFS is a a file system that is designed to work\n",
    "effectively with the Hadoop environment. In a typical Hadoop cluster,\n",
    "files would be broken up and distributed to different Hadoop nodes. The\n",
    "processing is moved to the data in this model, which can produce high\n",
    "throughput, especially for map/reduce programming tasks. However, this\n",
    "means you can not simply move around the HDFS file system in the same\n",
    "manner as a traditional Unix file system, since the components of a\n",
    "particular file are not all collocated. Instead, we must use the [HDFS\n",
    "file system interface][hdfs], which is invoked by using\n",
    "`$HADOOP_PREFIX/bin/hdfs`. Running this command by itself in our Hadoop\n",
    "Docker container will list the available commands, as shown in the\n",
    "following code cell.\n",
    "\n",
    "-----\n",
    "\n",
    "[hdfs]: https://hadoop.apache.org/docs/r2.7.2/hadoop-project-dist/hadoop-hdfs/HDFSCommands.html#dfs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: hdfs [--config confdir] [--loglevel loglevel] COMMAND\r\n",
      "       where COMMAND is one of:\r\n",
      "  dfs                  run a filesystem command on the file systems supported in Hadoop.\r\n",
      "  classpath            prints the classpath\r\n",
      "  namenode -format     format the DFS filesystem\r\n",
      "  secondarynamenode    run the DFS secondary namenode\r\n",
      "  namenode             run the DFS namenode\r\n",
      "  journalnode          run the DFS journalnode\r\n",
      "  zkfc                 run the ZK Failover Controller daemon\r\n",
      "  datanode             run a DFS datanode\r\n",
      "  dfsadmin             run a DFS admin client\r\n",
      "  haadmin              run a DFS HA admin client\r\n",
      "  fsck                 run a DFS filesystem checking utility\r\n",
      "  balancer             run a cluster balancing utility\r\n",
      "  jmxget               get JMX exported values from NameNode or DataNode.\r\n",
      "  mover                run a utility to move block replicas across\r\n",
      "                       storage types\r\n",
      "  oiv                  apply the offline fsimage viewer to an fsimage\r\n",
      "  oiv_legacy           apply the offline fsimage viewer to an legacy fsimage\r\n",
      "  oev                  apply the offline edits viewer to an edits file\r\n",
      "  fetchdt              fetch a delegation token from the NameNode\r\n",
      "  getconf              get config values from configuration\r\n",
      "  groups               get the groups which users belong to\r\n",
      "  snapshotDiff         diff two snapshots of a directory or diff the\r\n",
      "                       current directory contents with a snapshot\r\n",
      "  lsSnapshottableDir   list all snapshottable dirs owned by the current user\r\n",
      "\t\t\t\t\t\tUse -help to see options\r\n",
      "  portmap              run a portmap service\r\n",
      "  nfs3                 run an NFS version 3 gateway\r\n",
      "  cacheadmin           configure the HDFS cache\r\n",
      "  crypto               configure HDFS encryption zones\r\n",
      "  storagepolicies      list/get/set block storage policies\r\n",
      "  version              print the version\r\n",
      "\r\n",
      "Most commands print help when invoked w/o parameters.\r\n"
     ]
    }
   ],
   "source": [
    "!$HADOOP_PREFIX/bin/hdfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "The standard command we will use is `dfs` which runs a filesystem\n",
    "command on the HDFS file system that is supported by Hadoop. The [list\n",
    "of supported `dfs` commands][dfsl] is extensive, and mirrors many of the\n",
    "traditional Unix file systems commands. The full listing can be obtained\n",
    "by entering `$HADOOP_PREFIX/bin/hdfs dfs` at the prompt in our Hadoop\n",
    "Docker container, as shown below.\n",
    "\n",
    "-----\n",
    "\n",
    "[dfsl]: https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/FileSystemShell.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: hadoop fs [generic options]\n",
      "\t[-appendToFile <localsrc> ... <dst>]\n",
      "\t[-cat [-ignoreCrc] <src> ...]\n",
      "\t[-checksum <src> ...]\n",
      "\t[-chgrp [-R] GROUP PATH...]\n",
      "\t[-chmod [-R] <MODE[,MODE]... | OCTALMODE> PATH...]\n",
      "\t[-chown [-R] [OWNER][:[GROUP]] PATH...]\n",
      "\t[-copyFromLocal [-f] [-p] [-l] <localsrc> ... <dst>]\n",
      "\t[-copyToLocal [-p] [-ignoreCrc] [-crc] <src> ... <localdst>]\n",
      "\t[-count [-q] [-h] <path> ...]\n",
      "\t[-cp [-f] [-p | -p[topax]] <src> ... <dst>]\n",
      "\t[-createSnapshot <snapshotDir> [<snapshotName>]]\n",
      "\t[-deleteSnapshot <snapshotDir> <snapshotName>]\n",
      "\t[-df [-h] [<path> ...]]\n",
      "\t[-du [-s] [-h] <path> ...]\n",
      "\t[-expunge]\n",
      "\t[-find <path> ... <expression> ...]\n",
      "\t[-get [-p] [-ignoreCrc] [-crc] <src> ... <localdst>]\n",
      "\t[-getfacl [-R] <path>]\n",
      "\t[-getfattr [-R] {-n name | -d} [-e en] <path>]\n",
      "\t[-getmerge [-nl] <src> <localdst>]\n",
      "\t[-help [cmd ...]]\n",
      "\t[-ls [-d] [-h] [-R] [<path> ...]]\n",
      "\t[-mkdir [-p] <path> ...]\n",
      "\t[-moveFromLocal <localsrc> ... <dst>]\n",
      "\t[-moveToLocal <src> <localdst>]\n",
      "\t[-mv <src> ... <dst>]\n",
      "\t[-put [-f] [-p] [-l] <localsrc> ... <dst>]\n",
      "\t[-renameSnapshot <snapshotDir> <oldName> <newName>]\n",
      "\t[-rm [-f] [-r|-R] [-skipTrash] <src> ...]\n",
      "\t[-rmdir [--ignore-fail-on-non-empty] <dir> ...]\n",
      "\t[-setfacl [-R] [{-b|-k} {-m|-x <acl_spec>} <path>]|[--set <acl_spec> <path>]]\n",
      "\t[-setfattr {-n name [-v value] | -x name} <path>]\n",
      "\t[-setrep [-R] [-w] <rep> <path> ...]\n",
      "\t[-stat [format] <path> ...]\n",
      "\t[-tail [-f] <file>]\n",
      "\t[-test -[defsz] <path>]\n",
      "\t[-text [-ignoreCrc] <src> ...]\n",
      "\t[-touchz <path> ...]\n",
      "\t[-truncate [-w] <length> <path> ...]\n",
      "\t[-usage [cmd ...]]\n",
      "\n",
      "Generic options supported are\n",
      "-conf <configuration file>     specify an application configuration file\n",
      "-D <property=value>            use value for given property\n",
      "-fs <local|namenode:port>      specify a namenode\n",
      "-jt <local|resourcemanager:port>    specify a ResourceManager\n",
      "-files <comma separated list of files>    specify comma separated files to be copied to the map reduce cluster\n",
      "-libjars <comma separated list of jars>    specify comma separated jar files to include in the classpath.\n",
      "-archives <comma separated list of archives>    specify comma separated archives to be unarchived on the compute machines.\n",
      "\n",
      "The general command line syntax is\n",
      "bin/hadoop command [genericOptions] [commandOptions]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!$HADOOP_PREFIX/bin/hdfs dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    " Some of the more useful commands for this class\n",
    "include:\n",
    "\n",
    "- `-cat`: copies the source path to STDOUT.\n",
    "\n",
    "- `-count -h`: counts the number of directories, files and byts under the\n",
    "path specified. With the `-h` flag, the output is displayed in a\n",
    "human-readable format.\n",
    "\n",
    "- `-expunge`: empties the trash. By default, files and directories are\n",
    "not removed from HDFS with the `rm` command, they are simply moved to the\n",
    "trash. This can be useful when HDFS supplies a `Name node is in safe\n",
    "mode.` message. \n",
    "\n",
    "- `-ls`: lists the contents of the indicated directory in HDFS.\n",
    "\n",
    "- `-mkdir -p`: creates a new directory in HDFS at the specified\n",
    "location. With the `-p` flag any parent directory specified in the full\n",
    "path will also be created as necessary.\n",
    "\n",
    "- `-put`: copies indicated file(s) from local host file system into the\n",
    "specified path in HDFS.\n",
    "\n",
    "- `-rm -f -r`: delete the indicated file or directory. With the `-r -f`\n",
    "flags, the command will not display any message and any will delete any\n",
    "files or directories under the indicated directory. The `-skipTrash`\n",
    "flag should be used to delete the indicated resource immediately.\n",
    "\n",
    "- `-tail`: display the last kilobyte of the indicated file to STDOUT.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\n",
      "drwxrwx---   - data_scientist supergroup          0 2016-04-09 18:55 /tmp\n",
      "drwxr-xr-x   - data_scientist supergroup          0 2016-04-06 08:34 /user\n"
     ]
    }
   ],
   "source": [
    "# Display HDFS root directory structure\n",
    "!$HADOOP_PREFIX/bin/hdfs dfs -ls /"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 items\r\n",
      "drwxr-xr-x   - data_scientist supergroup          0 2016-04-06 08:34 /user/data_scientist\r\n"
     ]
    }
   ],
   "source": [
    "# Display HDFS directory\n",
    "!$HADOOP_PREFIX/bin/hdfs dfs -ls /user/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls: cannot access /user: No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "# Not a local filesystem directory so we get an error\n",
    "!ls /user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filesystem                  Size  Used  Available  Use%\r\n",
      "hdfs://a01f005a25fc:9000  18.2 G  28 K     11.1 G    0%\r\n"
     ]
    }
   ],
   "source": [
    "# Free Space\n",
    "!$HADOOP_PREFIX/bin/hdfs dfs -df -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "### Hadoop Example\n",
    "\n",
    "We can now turn to a complete Hadoop example. This example will run the\n",
    "`grep` command over a set of input files to search for the occurrences of\n",
    "a particular regular expression, which in this case is the three\n",
    "character sequence _dfs_ followed by one or more lowercase characters or\n",
    "a period. Hadoop tasks read inout from the HDFS filesystem and write\n",
    "their output to the HDFS file system. Thus we need to create an input\n",
    "directory, move our data to this input directory, before we can execute\n",
    "our Hadoop task. \n",
    "\n",
    "As demonstrated in the following code cell, we can do this easily with\n",
    "HDFS commands, after which we execute our specific Hadoop task. Notice\n",
    "how we include the input and output directories as part of the task\n",
    "execution. In some cases these will be indicated by flags. Finally, we\n",
    "display the directory contents, which demonstrate the successful\n",
    "completion of this task. Since Hadoop tasks can involve complex\n",
    "operations over a distributed file system, Hadoop tasks, by default,\n",
    "display a considerably quantity of information. You can capture the\n",
    "STDERR of any Hadoop (or HDFS) command to hide these informational\n",
    "messages. Of course, this will also hide any error messages, so proceed\n",
    "carefully if employing this technique.\n",
    "\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hadoop Directory\n",
      "Found 2 items\n",
      "drwxr-xr-x   - data_scientist supergroup          0 2016-04-09 18:56 hadoop/input\n",
      "drwxr-xr-x   - data_scientist supergroup          0 2016-04-09 18:57 hadoop/output\n",
      "\n",
      "Hadoop Input Directory\n",
      "Found 9 items\n",
      "-rw-r--r--   1 data_scientist supergroup       4436 2016-04-09 18:56 hadoop/input/capacity-scheduler.xml\n",
      "-rw-r--r--   1 data_scientist supergroup        158 2016-04-09 18:56 hadoop/input/core-site.xml\n",
      "-rw-r--r--   1 data_scientist supergroup       9683 2016-04-09 18:56 hadoop/input/hadoop-policy.xml\n",
      "-rw-r--r--   1 data_scientist supergroup        354 2016-04-09 18:56 hadoop/input/hdfs-site.xml\n",
      "-rw-r--r--   1 data_scientist supergroup        620 2016-04-09 18:56 hadoop/input/httpfs-site.xml\n",
      "-rw-r--r--   1 data_scientist supergroup       3518 2016-04-09 18:56 hadoop/input/kms-acls.xml\n",
      "-rw-r--r--   1 data_scientist supergroup       5511 2016-04-09 18:56 hadoop/input/kms-site.xml\n",
      "-rw-r--r--   1 data_scientist supergroup        357 2016-04-09 18:56 hadoop/input/mapred-site.xml\n",
      "-rw-r--r--   1 data_scientist supergroup       1525 2016-04-09 18:56 hadoop/input/yarn-site.xml\n",
      "\n",
      "Hadoop Output Directory\n",
      "Found 2 items\n",
      "-rw-r--r--   1 data_scientist supergroup          0 2016-04-09 18:57 hadoop/output/_SUCCESS\n",
      "-rw-r--r--   1 data_scientist supergroup         74 2016-04-09 18:57 hadoop/output/part-r-00000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16/04/09 18:56:36 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/04/09 18:56:37 INFO input.FileInputFormat: Total input paths to process : 9\n",
      "16/04/09 18:56:37 INFO mapreduce.JobSubmitter: number of splits:9\n",
      "16/04/09 18:56:37 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1460228114594_0001\n",
      "16/04/09 18:56:38 INFO impl.YarnClientImpl: Submitted application application_1460228114594_0001\n",
      "16/04/09 18:56:38 INFO mapreduce.Job: The url to track the job: http://a01f005a25fc:8088/proxy/application_1460228114594_0001/\n",
      "16/04/09 18:56:38 INFO mapreduce.Job: Running job: job_1460228114594_0001\n",
      "16/04/09 18:56:48 INFO mapreduce.Job: Job job_1460228114594_0001 running in uber mode : false\n",
      "16/04/09 18:56:48 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/04/09 18:57:12 INFO mapreduce.Job:  map 33% reduce 0%\n",
      "16/04/09 18:57:13 INFO mapreduce.Job:  map 67% reduce 0%\n",
      "16/04/09 18:57:30 INFO mapreduce.Job:  map 89% reduce 0%\n",
      "16/04/09 18:57:31 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/04/09 18:57:32 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/04/09 18:57:32 INFO mapreduce.Job: Job job_1460228114594_0001 completed successfully\n",
      "16/04/09 18:57:32 INFO mapreduce.Job: Counters: 50\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=112\n",
      "\t\tFILE: Number of bytes written=1187337\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=27401\n",
      "\t\tHDFS: Number of bytes written=216\n",
      "\t\tHDFS: Number of read operations=30\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tKilled map tasks=1\n",
      "\t\tLaunched map tasks=10\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=10\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=183129\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=15657\n",
      "\t\tTotal time spent by all map tasks (ms)=183129\n",
      "\t\tTotal time spent by all reduce tasks (ms)=15657\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=183129\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=15657\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=187524096\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=16032768\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=749\n",
      "\t\tMap output records=4\n",
      "\t\tMap output bytes=98\n",
      "\t\tMap output materialized bytes=160\n",
      "\t\tInput split bytes=1239\n",
      "\t\tCombine input records=4\n",
      "\t\tCombine output records=4\n",
      "\t\tReduce input groups=4\n",
      "\t\tReduce shuffle bytes=160\n",
      "\t\tReduce input records=4\n",
      "\t\tReduce output records=4\n",
      "\t\tSpilled Records=8\n",
      "\t\tShuffled Maps =9\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=9\n",
      "\t\tGC time elapsed (ms)=3083\n",
      "\t\tCPU time spent (ms)=4050\n",
      "\t\tPhysical memory (bytes) snapshot=1830297600\n",
      "\t\tVirtual memory (bytes) snapshot=19209564160\n",
      "\t\tTotal committed heap usage (bytes)=1269469184\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=26162\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=216\n",
      "16/04/09 18:57:32 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/04/09 18:57:32 INFO input.FileInputFormat: Total input paths to process : 1\n",
      "16/04/09 18:57:32 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "16/04/09 18:57:32 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1460228114594_0002\n",
      "16/04/09 18:57:32 INFO impl.YarnClientImpl: Submitted application application_1460228114594_0002\n",
      "16/04/09 18:57:32 INFO mapreduce.Job: The url to track the job: http://a01f005a25fc:8088/proxy/application_1460228114594_0002/\n",
      "16/04/09 18:57:32 INFO mapreduce.Job: Running job: job_1460228114594_0002\n",
      "16/04/09 18:57:45 INFO mapreduce.Job: Job job_1460228114594_0002 running in uber mode : false\n",
      "16/04/09 18:57:45 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/04/09 18:57:50 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/04/09 18:57:57 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/04/09 18:57:57 INFO mapreduce.Job: Job job_1460228114594_0002 completed successfully\n",
      "16/04/09 18:57:57 INFO mapreduce.Job: Counters: 49\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=112\n",
      "\t\tFILE: Number of bytes written=236571\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=359\n",
      "\t\tHDFS: Number of bytes written=74\n",
      "\t\tHDFS: Number of read operations=7\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=1\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=1\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=3154\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=3735\n",
      "\t\tTotal time spent by all map tasks (ms)=3154\n",
      "\t\tTotal time spent by all reduce tasks (ms)=3735\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=3154\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=3735\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=3229696\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=3824640\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=4\n",
      "\t\tMap output records=4\n",
      "\t\tMap output bytes=98\n",
      "\t\tMap output materialized bytes=112\n",
      "\t\tInput split bytes=143\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=1\n",
      "\t\tReduce shuffle bytes=112\n",
      "\t\tReduce input records=4\n",
      "\t\tReduce output records=4\n",
      "\t\tSpilled Records=8\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=138\n",
      "\t\tCPU time spent (ms)=890\n",
      "\t\tPhysical memory (bytes) snapshot=306745344\n",
      "\t\tVirtual memory (bytes) snapshot=3848744960\n",
      "\t\tTotal committed heap usage (bytes)=170004480\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=216\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=74\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "# Example derived from Hadoop single node setup documentation:\n",
    "#\n",
    "# https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/SingleCluster.html\n",
    "#\n",
    "\n",
    "cd $HADOOP_PREFIX\n",
    "\n",
    "# Remove old directory (if it exsits) to have clean example\n",
    "bin/hdfs dfs -rm -r -f hadoop\n",
    "\n",
    "# Make directorties for example application\n",
    "bin/hdfs dfs -mkdir -p hadoop\n",
    "bin/hdfs dfs -mkdir -p hadoop/input\n",
    "\n",
    "# Copy data into example input directory\n",
    "bin/hdfs dfs -put etc/hadoop/*.xml hadoop/input\n",
    "\n",
    "# Running Hadoop example to test installation\n",
    "example_file=$(ls share/hadoop/mapreduce/hadoop-mapreduce-examples*)\n",
    "bin/hadoop jar $example_file grep hadoop/input hadoop/output 'dfs[a-z.]+'\n",
    "\n",
    "# Display directory heirarchy\n",
    "echo\n",
    "echo 'Hadoop Directory'\n",
    "bin/hdfs dfs -ls hadoop/\n",
    "\n",
    "echo\n",
    "echo 'Hadoop Input Directory'\n",
    "bin/hdfs dfs -ls hadoop/input\n",
    "\n",
    "echo\n",
    "echo 'Hadoop Output Directory'\n",
    "bin/hdfs dfs -ls hadoop/output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "As shown in the output directory display, several files were created by\n",
    "our Hadoop task. The first file, `_SUCCESS`, is self-explanatory,\n",
    "especially when you see the file is empty. The second file,\n",
    "`part-r-00000` contains the output of the command. We can display the\n",
    "contents of this file by using the HDFS `cat` command as shown in the\n",
    "following cell.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\tdfsadmin\r\n",
      "1\tdfs.replication\r\n",
      "1\tdfs.namenode.servicerpc\r\n",
      "1\tdfs.namenode.rpc\r\n"
     ]
    }
   ],
   "source": [
    "!$HADOOP_PREFIX/bin/hdfs dfs -cat hadoop/output/part-r-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "Of course, we want to know if this output is correct. To test this, we\n",
    "can use standard Unix `grep` command to find the expected output, which\n",
    "shows the same four lines (albeit we don't use a full regular\n",
    "expression, so the whole line is displayed by default).\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[35m\u001b[K/usr/local/hadoop/etc/hadoop/hadoop-policy.xml\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K    \u001b[01;31m\u001b[Kdfsa\u001b[m\u001b[Kdmin and mradmin commands to refresh the security policy in-effect.\r\n",
      "\u001b[35m\u001b[K/usr/local/hadoop/etc/hadoop/hdfs-site.xml\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K        <name>\u001b[01;31m\u001b[Kdfs.\u001b[m\u001b[Kreplication</name>\r\n",
      "\u001b[35m\u001b[K/usr/local/hadoop/etc/hadoop/hdfs-site.xml\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K        <name>\u001b[01;31m\u001b[Kdfs.\u001b[m\u001b[Knamenode.rpc-bind-host</name>\r\n",
      "\u001b[35m\u001b[K/usr/local/hadoop/etc/hadoop/hdfs-site.xml\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K        <name>\u001b[01;31m\u001b[Kdfs.\u001b[m\u001b[Knamenode.servicerpc-bind-host</name>\r\n"
     ]
    }
   ],
   "source": [
    "!grep --color 'dfs[a-z.]' $HADOOP_PREFIX/etc/hadoop/*.xml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "## Acquiring Data \n",
    "\n",
    "In the next two lessons, we will analyze text data by using Hadoop\n",
    "map-reduce and Pig. As a result, we finish this Notebook by acquiring a\n",
    "text data set for later analysis, which we stage locally (i.e., outside\n",
    "HDFS). First, we delete our local directory if it exists and create it\n",
    "to have a clean install.\n",
    "\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 8\n",
      "drwxr-xr-x  2 data_scientist users 4096 Apr  9 18:58 .\n",
      "drwxr-xr-x 18 data_scientist users 4096 Apr  9 18:58 ..\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "#!/usr/bin/env bash\n",
    "# A Bash Shell Script to delete the Hadoop diorectory if it exists, afterwhich\n",
    "# make a new Hadoop directory\n",
    "\n",
    "# Our directory name\n",
    "DIR=$HOME/hadoop\n",
    "\n",
    "# Delete if exists\n",
    "if [ -d \"$DIR\" ]; then\n",
    "    rm -rf \"$DIR\"\n",
    "fi\n",
    "\n",
    "# Now make the directory\n",
    "mkdir \"$DIR\"\n",
    "\n",
    "ls -la $DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "### Acquiring Data\n",
    "\n",
    "To perform data analysis by using Hadoop, we will need a data set. In the\n",
    "Notebook for the second lesson this week, we will perform a simple\n",
    "map/reduce operation that will require text data to operate. While there\n",
    "are a number of possible options, for this example we can grab a free\n",
    "book from [Project Gutenberg][pg]:\n",
    "\n",
    "    wget --directory-prefix=$HOME/hadoop/ --output-document=book.txt \\\n",
    "        http://www.gutenberg.org/cache/epub/4300/pg4300.txt`\n",
    "\n",
    "In this case, we have grabbed the full text of the novel _Ulysses_, by\n",
    "James Joyce, and placed the text in the `hadoop` subdirectory of our\n",
    "_home_ directory.\n",
    "\n",
    "-----\n",
    "[pg]: http://www.gutenberg.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2016-04-09 18:58:08--  http://www.gutenberg.org/cache/epub/4300/pg4300.txt\n",
      "Resolving www.gutenberg.org (www.gutenberg.org)... 152.19.134.47\n",
      "Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1573151 (1.5M) [text/plain]\n",
      "Saving to: ‘/home/data_scientist/hadoop/book.txt’\n",
      "\n",
      "100%[======================================>] 1,573,151   1.11MB/s   in 1.4s   \n",
      "\n",
      "2016-04-09 18:58:10 (1.11 MB/s) - ‘/home/data_scientist/hadoop/book.txt’ saved [1573151/1573151]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Grab a book to process\n",
    "!wget --output-document=$HOME/hadoop/book.txt \\\n",
    "http://www.gutenberg.org/cache/epub/4300/pg4300.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "At this point, we first need to create an directory to hold the input\n",
    "and output of our Hadoop task. We will create a new directory called\n",
    "`wc` with a subdirectory called `in` to hold the input data for our\n",
    "Hadoop task. Second, we will need to copy the book text file into this\n",
    "new HDFS directory. This means we will need to run the following two\n",
    "Hadoop commands:\n",
    "\n",
    "1. `$HADOOP_PREFIX/bin/hdfs dfs -mkdir -p wc/in`\n",
    "2. `$HADOOP_PREFIX/bin/hdfs dfs -put book.txt wc/in/book.txt`\n",
    "\n",
    "The following cell contains these commands (and two other commands) the\n",
    "result of running these two commands, as well as the `dfs -ls` command\n",
    "to display the contents of our new HDFS directory, and the `dfs -count`\n",
    "command to show the size of the directory contents. At the end of this\n",
    "output will be a message from Hadoop, which simply states that files are\n",
    "being immediately deleted. This value can be changed to cache files\n",
    "before deleting for a specific time interval, which would, of course,\n",
    "allow files to be recovered if accidentally deleted.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating input directory, and copying data.\n",
      "\n",
      "Input directory contents\n",
      "           0            1              1.5 M wc/in/book.txt\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "cd $HADOOP_PREFIX\n",
    "\n",
    "bin/hdfs dfs -rm -r -f wc\n",
    "\n",
    "echo\n",
    "echo 'Creating input directory, and copying data.'\n",
    "bin/hdfs dfs -mkdir -p wc/in\n",
    "bin/hdfs dfs -put $HOME/hadoop/book.txt wc/in/book.txt\n",
    "\n",
    "echo\n",
    "echo 'Input directory contents'\n",
    "bin/hdfs dfs -count -h wc/in/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "### Student Activity\n",
    "\n",
    "In the preceding cells, we introduced Hadoop and the HDFS file system.\n",
    "Now that you have run the Notebook, go back and make the following\n",
    "changes to see how the results change.\n",
    "\n",
    "1. Create a new Hadoop HDFS directory, use your own name for the\n",
    "directory name.\n",
    "2. Copy one or more local files into your new Hadoop directory. Run a\n",
    "Hadoop command to display the files and their byte count, do the results\n",
    "agree with your local values?\n",
    "3. Run a different `grep` example on the book you downloaded. Do the\n",
    "results make sense?\n",
    "\n",
    "-----"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
