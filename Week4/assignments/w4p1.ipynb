{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "e86311be21f81c722c21243bd984de5f",
     "grade": false,
     "grade_id": "header",
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "This notebook will be collected automatically at **6pm on Monday** from `/home/data_scientist/assignments/Week4` directory on the course JupyterHub server. If you work on this assignment on the course Jupyterhub server, just make sure that you save your work and instructors will pull your notebooks automatically after the deadline. If you work on this assignment locally, the only way to submit assignments is via Jupyterhub, and you have to place the notebook file in the correct directory with the correct file name before the deadline.\n",
    "\n",
    "1. Make sure everything runs as expected. First, restart the kernel (in the menubar, select `Kernel` → `Restart`) and then run all cells (in the menubar, select `Cell` → `Run All`).\n",
    "2. Make sure you fill in any place that says `YOUR CODE HERE`. Do not write your answer in anywhere else other than where it says `YOUR CODE HERE`. Anything you write anywhere else will be removed by the autograder.\n",
    "3. Do not change the file path or the file name of this notebook.\n",
    "4. Make sure that you save your work (in the menubar, select `File` → `Save and CheckPoint`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 4.1. Decision Trees\n",
    "\n",
    "In this problem, we will use the Decision Trees algorithm to see if we can use machine learning techniques to predict departure delays at the O'Hare airport (ORD).\n",
    "\n",
    "A bit of introduction before we begin. You will see that this problem is not really about decision trees but data preparation. However, it is what practical data science is really about; the actual machine learning, especially with packages like scikit-learn, is a line of `fit` and `predict`. The rest is data munging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "be7953bdb303181791e933655af4366a",
     "grade": false,
     "grade_id": "import",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "import time\n",
    "import requests\n",
    "import json\n",
    "from pprint import pprint\n",
    "\n",
    "from sklearn import tree\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.utils import check_random_state\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from nose.tools import assert_equal, assert_is_not, assert_is_instance\n",
    "from numpy.testing import assert_array_equal, assert_array_almost_equal, assert_almost_equal\n",
    "from pandas.util.testing import assert_frame_equal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose you want to include weather as training features. `2001.csv` doesn't have weather information, so we have to gather the data ourselves. There are various weather APIs available, one of which is [Weather Underground](http://www.wunderground.com/). Their terms of service says I have to display their logo, so here it is:\n",
    "\n",
    "![](http://www.wunderground.com/logos/images/wundergroundLogo_4c.jpg)\n",
    "\n",
    "After you sign up for an account and generate an API token, you can issue HTTP requests such as:\n",
    "\n",
    "```\n",
    "http://api.wunderground.com/api/<token number>/history_20010101/conditions/q/KORD.json\n",
    "```\n",
    "\n",
    "The above example will return a JSON with historical weather information on January 1, 2001 (20010101) at O'Hare (KORD). To save you the trouble of dealing with the Weather Underground API, I saved the JSON responses as `.json` files in `/home/data_scientist/data/weather`.\n",
    "\n",
    "```shell\n",
    "$ ls /home/data_scientist/data/weather | head\n",
    "```\n",
    "\n",
    "```\n",
    "weather_kord_2001_0101.json\n",
    "weather_kord_2001_0102.json\n",
    "weather_kord_2001_0103.json\n",
    "weather_kord_2001_0104.json\n",
    "weather_kord_2001_0105.json\n",
    "weather_kord_2001_0106.json\n",
    "weather_kord_2001_0107.json\n",
    "weather_kord_2001_0108.json\n",
    "weather_kord_2001_0109.json\n",
    "weather_kord_2001_0110.json\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "7be4c8dca7a1b7d2eb55863bf752288f",
     "grade": false,
     "grade_id": "ls_weather",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "!ls /home/data_scientist/data/weather | head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each file contains exactly the same response you would get from the Weather Underground API, because I simply dumped the JSON responses to files. Here is the full code that generated these files:\n",
    "\n",
    "```python\n",
    "def get_2001_json(date, year=2001):\n",
    "    url = 'http://api.wunderground.com/api/e693d410bdf457e2/history_{0}{1}/conditions/q/KORD.json'.format(year, date)\n",
    "    resp = requests.get(url)\n",
    "    resp_json = resp.json()\n",
    "    return resp_json\n",
    "\n",
    "def save_2001_json(date, dir_name='data', filename='weather_kord_2001_{}.json'):\n",
    "    data = get_2001_json(date)\n",
    "    path = os.path.join(dir_name, filename.format(date))\n",
    "    with open(path, 'w') as f:\n",
    "        json.dump(data, f)\n",
    "\n",
    "dates = ['{0:0>2}{1:0>2}'.format(m, d) for m in [1, 3, 5, 7, 8, 10, 12] for d in range(1, 32)]\n",
    "dates.extend(['{0:0>2}{1:0>2}'.format(m, d) for m in [4, 6, 9, 11] for d in range(1, 31)])\n",
    "dates.extend(['02{0:0>2}'.format(d) for d in range(1, 29)])\n",
    "\n",
    "if not os.path.exists('data'):\n",
    "    os.mkdir('data')\n",
    "\n",
    "for d in dates:\n",
    "    save_2001_json(d)\n",
    "    time.sleep(6) # free plan limit: 10 calls/min\n",
    "```\n",
    "\n",
    "Do not run this code to generate these files. We will use the files in `/home/data_scientist/data/weather` instead.\n",
    "\n",
    "## Load JSON files\n",
    "\n",
    "- Write a function named `from_json_to_dict()` that takes a string in the format `MMDD` (M = Month, D = Day of month) and returns a dictoinary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "5e657fd6a633cb0d91856d986a629f36",
     "grade": false,
     "grade_id": "from_json_to_dict_answer",
     "locked": false,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def from_json_to_dict(date, path='/home/data_scientist/data/weather/', prefix='weather_kord_2001_'):\n",
    "    '''\n",
    "    Takes a string in the format MMDD where M = month, D = day of month.\n",
    "    Read a json file at \"path\" + \"prefix\" + \"date\".\n",
    "    Returns the JSON dictionary.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    date: A string.\n",
    "    \n",
    "    Optional\n",
    "    --------\n",
    "    path: A string.\n",
    "    prefix: A string.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    A dict.\n",
    "    '''\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tests for `from_json_to_dict()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "2933eb735de035b64ccc397bcce9a472",
     "grade": true,
     "grade_id": "from_json_to_dict_test",
     "locked": true,
     "points": 5,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "test_0101_dict = from_json_to_dict('0101')\n",
    "assert_is_instance(test_0101_dict, dict)\n",
    "assert_equal('history' in test_0101_dict, True)\n",
    "assert_equal('observations' in test_0101_dict['history'], True)\n",
    "assert_is_instance(test_0101_dict['history']['observations'], list)\n",
    "\n",
    "test_0103_dict = from_json_to_dict('0103')\n",
    "assert_is_instance(test_0103_dict, dict)\n",
    "assert_equal('history' in test_0103_dict, True)\n",
    "assert_equal('observations' in test_0103_dict['history'], True)\n",
    "assert_is_instance(test_0103_dict['history']['observations'], list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse time and visibility from JSON\n",
    "\n",
    "- Write a function named `from_dict_to_visibility()` that takes a dictionary and returns a list of 5-tuples of the form `(Month, Day, Hour, Minute, Visibility)`.\n",
    "\n",
    "We covered the json format in the previous course, so you know how to do this. Let's say you created a dictionary called `data` by reading the json file `weather_kord_2001_0101.json`.\n",
    "\n",
    "```python\n",
    ">>> data = from_json_to_dict('0101')\n",
    ">>> print(data.keys()\n",
    "```\n",
    "\n",
    "```\n",
    "dict_keys(['response', 'current_observation', 'history'])\n",
    "```\n",
    "\n",
    "You can peek into `response` and `current_observation` but they are not important for our purposes, so we look at `history`:\n",
    "\n",
    "```python\n",
    ">>> print(data['history'].keys())\n",
    "```\n",
    "\n",
    "```\n",
    "dict_keys(['observations', 'date', 'dailysummary', 'utcdate'])\n",
    "```\n",
    "\n",
    "Here, `observations` is a list.\n",
    "\n",
    "```python\n",
    ">>> print(type(data['history']['observations']))\n",
    "```\n",
    "\n",
    "```\n",
    "<class 'list'>\n",
    "```\n",
    "\n",
    "The first element looks like as follows:\n",
    "\n",
    "```python\n",
    ">>> from pprint import pprint\n",
    ">>> pprint(data['history']['observations'][0])\n",
    "```\n",
    "\n",
    "```\n",
    "{'conds': 'Overcast',\n",
    " 'date': {'hour': '00',\n",
    "          'mday': '01',\n",
    "          'min': '56',\n",
    "          'mon': '01',\n",
    "          'pretty': '12:56 AM CST on January 01, 2001',\n",
    "          'tzname': 'America/Chicago',\n",
    "          'year': '2001'},\n",
    " 'dewpti': '10.9',\n",
    " 'dewptm': '-11.7',\n",
    " 'fog': '0',\n",
    " 'hail': '0',\n",
    " 'heatindexi': '-9999',\n",
    " 'heatindexm': '-9999',\n",
    " 'hum': '92',\n",
    " 'icon': 'cloudy',\n",
    " 'metar': 'METAR KORD 010656Z 36004KT 9SM BKN055 OVC095 M11/M12 A3034 RMK '\n",
    "          'AO2 SLP285 T11061117 $',\n",
    " 'precipi': '-9999.00',\n",
    " 'precipm': '-9999.00',\n",
    " 'pressurei': '30.38',\n",
    " 'pressurem': '1028.5',\n",
    " 'rain': '0',\n",
    " 'snow': '0',\n",
    " 'tempi': '12.9',\n",
    " 'tempm': '-10.6',\n",
    " 'thunder': '0',\n",
    " 'tornado': '0',\n",
    " 'utcdate': {'hour': '06',\n",
    "             'mday': '01',\n",
    "             'min': '56',\n",
    "             'mon': '01',\n",
    "             'pretty': '6:56 AM GMT on January 01, 2001',\n",
    "             'tzname': 'UTC',\n",
    "             'year': '2001'},\n",
    " 'visi': '9.0',\n",
    " 'vism': '14.5',\n",
    " 'wdird': '360',\n",
    " 'wdire': 'North',\n",
    " 'wgusti': '-9999.0',\n",
    " 'wgustm': '-9999.0',\n",
    " 'windchilli': '5.2',\n",
    " 'windchillm': '-14.9',\n",
    " 'wspdi': '4.6',\n",
    " 'wspdm': '7.4'}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "d440a68d8c955fdb7e7f697f4314e30e",
     "grade": false,
     "grade_id": "from_dict_to_visibility",
     "locked": false,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def from_dict_to_visibility(json_data):\n",
    "    '''\n",
    "    Takes a dictionary and returns a list of 5-tuples, (Month, Day, Hour, Minute, Visibility).\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    json_data: A dict.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    A list of 5-tuples (str, str, str, str, str)\n",
    "    '''\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tests for `from_dict_to_visibility()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "15c472222e1cc908f3df1b502952cf0f",
     "grade": true,
     "grade_id": "from_dict_to_visibility_test",
     "locked": true,
     "points": 10,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "test_0101_visi = from_dict_to_visibility(test_0101_dict)\n",
    "assert_is_instance(test_0101_visi, list)\n",
    "assert_equal(len(test_0101_visi), 24)\n",
    "for item in test_0101_visi:\n",
    "    assert_is_instance(item, tuple)\n",
    "    assert_equal(len(item), 5) # month, day, hour, minute, visibility\n",
    "    assert_equal(item[0], '01')\n",
    "    assert_equal(item[1], '01')\n",
    "    \n",
    "test_0103_visi = from_dict_to_visibility(test_0103_dict)\n",
    "assert_is_instance(test_0103_visi, list)\n",
    "assert_equal(len(test_0103_visi), 34) # some days have more than one measurement per hour\n",
    "for item in test_0103_visi:\n",
    "    assert_is_instance(item, tuple)\n",
    "    assert_equal(len(item), 5)\n",
    "    assert_equal(item[0], '01')\n",
    "    assert_equal(item[1], '03')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process all 365 files\n",
    "\n",
    "We will use the functions `from_json_to_dict()` and `from_dict_to_visibility()` (in a loop) for all 365 days of the year. Let's first generate a list of dates in sequential order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "3ac5e7f935647102d8e5853a94cbe927",
     "grade": false,
     "grade_id": "dates",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "dates = ['{0:0>2}{1:0>2}'.format(m, d + 1) for m in [1, 3, 5, 7, 8, 10, 12] for d in range(31)]\n",
    "dates.extend(['{0:0>2}{1:0>2}'.format(m, d + 1) for m in [4, 6, 9, 11] for d in range(30)])\n",
    "dates.extend(['02{0:0>2}'.format(d + 1) for d in range(28)])\n",
    "dates.sort()\n",
    "\n",
    "assert_equal(len(dates), 365)\n",
    "\n",
    "print(\"The first five elements are {}\".format(dates[:5]))\n",
    "print(\"The last five elements are {}\".format(dates[-5:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Write a function named `collect_365_days()` that takes a list of strings, iterates through the list, and uses `from_json_to_dict()` and `from_dict_to_visibility()` to return a list of 5-tuples `(month, day, hour, minute, visibility)`.\n",
    "\n",
    "Here's the output you should get:\n",
    "\n",
    "```python\n",
    ">>> visibilities = collect_365_days(dates)\n",
    ">>> print(\"The length of visibilities is {}.\".format(len(visibilities)))\n",
    ">>> print(\"The first five elements of visibilities are {}\".format(visibilities[:5]))\n",
    "```\n",
    "\n",
    "```\n",
    "The length of visibilities is 10159.\n",
    "The first five elements of visibilities are [('01', '01', '00', '56', '9.0'), ('01', '01', '01', '56', '7.0'), ('01', '01', '02', '56', '10.0'), ('01', '01', '03', '56', '10.0'), ('01', '01', '04', '56', '9.0')]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "5675f2613fb79691cff302cd7aa1f2f4",
     "grade": false,
     "grade_id": "collect_365_days_answer",
     "locked": false,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def collect_365_days(dates):\n",
    "    '''\n",
    "    Uses from_json_to_dict() and from_dict_to_visiblility() to\n",
    "    generate a list of tuples of the form\n",
    "    (Month, Day, Hour, Minute, Visibility)\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    dates: A list of strings \"MMDD\"\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    A list of 5-tuples (str, str, str, str, str)\n",
    "    '''\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "        \n",
    "    return visibilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "2183844ca1c40754094823c1c3bb385f",
     "grade": false,
     "grade_id": "collect_365_days_print",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "visibilities = collect_365_days(dates)\n",
    "\n",
    "print(\"The length of visibilities is {}.\".format(len(visibilities)))\n",
    "print(\"The first five elements of visibilities are {}\".format(visibilities[:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "341093cce86228696efbce84b46570ca",
     "grade": true,
     "grade_id": "collect_365_days_test",
     "locked": true,
     "points": 5,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert_is_instance(visibilities, list)\n",
    "assert_equal(len(visibilities), 10159)\n",
    "assert_equal(visibilities[:5],\n",
    "    [('01', '01', '00', '56', '9.0'),\n",
    "     ('01', '01', '01', '56', '7.0'),\n",
    "     ('01', '01', '02', '56', '10.0'),\n",
    "     ('01', '01', '03', '56', '10.0'),\n",
    "     ('01', '01', '04', '56', '9.0')]\n",
    "    )\n",
    "assert_equal(visibilities[-5:],\n",
    "    [('12', '31', '19', '56', '10.0'),\n",
    "     ('12', '31', '20', '56', '10.0'),\n",
    "     ('12', '31', '21', '56', '10.0'),\n",
    "     ('12', '31', '22', '56', '10.0'),\n",
    "     ('12', '31', '23', '56', '10.0')]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will combine the weather data with our flights data. We import the following columns of `2001.csv`:\n",
    "\n",
    "- Column 1: Month, 1-12\n",
    "- Column 2: DayofMonth, 1-31\n",
    "- Column 5: CRSDepTime, scheduled departure time (local, hhmm)\n",
    "- Column 8: UniqueCarrier, unique carrier code\n",
    "- Column 15: DepDelay, departure delay, in minutes\n",
    "- Column 16: Origin, origin IATA airport code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "703c8370875ed9945943d19e209df99c",
     "grade": false,
     "grade_id": "read_csv",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\n",
    "    '/home/data_scientist/data/2001.csv',\n",
    "    encoding='latin-1',\n",
    "    usecols=(1, 2, 5, 8, 15, 16)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use only AA flights that departed from ORD (American Airlines is the largest airline using the O'Hare airport). We define a flight to be delayed if its departure delay is 15 minutes or more, the same definition used by the FAA (source: [Wikipedia](https://en.wikipedia.org/wiki/Flight_cancellation_and_delay))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "511e7829f71cc3c0ab28c3bcaaf60ab6",
     "grade": false,
     "grade_id": "local",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "local = df[(df['Origin'] == 'ORD') & (df['UniqueCarrier'] == 'AA')]\n",
    "local = local.drop(['UniqueCarrier', 'Origin'], axis=1) # we don't need the Month and Origin columns anymore.\n",
    "local['Delayed'] = (local['DepDelay'] > 15).astype(np.int) # 1 if a flight was delayed, 0 if not.\n",
    "local = local.drop('DepDelay', axis=1).dropna() # we don't need the DepDelay column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's print the first few columns and see what we'll be working with.\n",
    "\n",
    "```python\n",
    ">>> print(local.head(5))\n",
    "```\n",
    "\n",
    "```\n",
    "      Month  DayofMonth  CRSDepTime  Delayed\n",
    "        Month  DayofMonth  CRSDepTime  Delayed\n",
    "398444      1           1        1905        1\n",
    "398445      1           2        1905        1\n",
    "398446      1           3        1905        1\n",
    "398447      1           4        1905        0\n",
    "398448      1           5        1905        1\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "a21dce3e8525d5c0130ef5fcc3f26831",
     "grade": false,
     "grade_id": "print_local_head",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "print(local.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert strings to numbers\n",
    "\n",
    "Now we want to match the `Month` and `DayofMonth` columns in `local` with the corresponding entries in `visibilities` and find the time in `visibilities` that is closes to the `CRSDepTime`. What would be the best way to about matching the times?\n",
    "\n",
    "Rahter than comparing three columns, I think it's better to combine the three numbers into one long number and compare just one column. Recall that we had a tuple of strings, while the data types in `local` is integer.\n",
    "\n",
    "```python\n",
    ">>> print(local.CRSDepTime.dtype)\n",
    "```\n",
    "\n",
    "```\n",
    "int64\n",
    "```\n",
    "\n",
    "So let's convert the strings into integers in the form `mmddHHMM`, where `m` is month, `d` is day of month, `H` is hour, and `M` is minute. Let's create a data frame from tuple while we are at it so our function can do:\n",
    "\n",
    "```python\n",
    ">>> print(visibilities[:3])\n",
    "```\n",
    "\n",
    "```\n",
    "[('01', '01', '00', '56', '9.0'), ('01', '01', '01', '56', '7.0'), ('01', '01', '02', '56', '10.0')]\n",
    "```\n",
    "\n",
    "```python\n",
    ">>> time_visi = from_string_to_numbers(visibilities)\n",
    ">>> print(time_visi.head(3))\n",
    "```\n",
    "\n",
    "```\n",
    "      Time  Visibility\n",
    "0  1010056           9\n",
    "1  1010156           7\n",
    "2  1010256          10\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "e24c52193b66bdd87093fc6061786092",
     "grade": false,
     "grade_id": "from_string_to_numbers_answer",
     "locked": false,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def from_string_to_numbers(visibilities):\n",
    "    '''\n",
    "    Takes a list of 5-tuples of strings.\n",
    "    Convert the strings into integers in the form `mmddHHMM`,\n",
    "    where `m` is month, `d` is day of month, `H` is hour, and `M` is minute.\n",
    "    Returns a pandas.DataFrame with two columns \"Time\" and \"Visibility\".\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    visibilities: A list of 5-tuple of strings.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    A pandas.DataFrame\n",
    "    '''\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "9ce89cf52370351b87e8683f66438815",
     "grade": false,
     "grade_id": "time_visi",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "time_visi = from_string_to_numbers(visibilities)\n",
    "print(time_visi.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "99c23128a688746e56ed01d33466acc8",
     "grade": true,
     "grade_id": "from_string_to_numbers_test",
     "locked": true,
     "points": 10,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "visi0 = [\n",
    "    ('01', '01', '06', '00', '1.0'),\n",
    "    ('02', '31', '08', '00', '2.0'),\n",
    "    ('10', '05', '07', '00', '3.0'),\n",
    "    ('12', '29', '09', '00', '4.0'),\n",
    "    ('09', '30', '23', '00', '5.0'),\n",
    "    ('07', '04', '12', '00', '6.0'),\n",
    "    ('05', '12', '15', '00', '7.0'),\n",
    "    ('11', '11', '18', '00', '8.0')\n",
    "]\n",
    "\n",
    "visi_answer = pd.DataFrame({\n",
    "    'Time': [1010600, 2310800, 10050700, 12290900,\n",
    "             9302300, 7041200, 5121500, 11111800],\n",
    "    'Visibility': [1., 2., 3., 4., 5., 6., 7., 8.]\n",
    "    })\n",
    "\n",
    "assert_frame_equal(from_string_to_numbers(visi0), visi_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Time column\n",
    "\n",
    "- Do the same for the `local` data frame. Put the result into a column named `Time` so we have\n",
    "\n",
    "```python\n",
    ">>> time_delayed = combine_time(local)\n",
    ">>> print(time_delayed.head())\n",
    "```\n",
    "\n",
    "```\n",
    "        Month  DayofMonth  CRSDepTime  Delayed     Time\n",
    "398444      1           1        1905        1  1011905\n",
    "398445      1           2        1905        1  1021905\n",
    "398446      1           3        1905        1  1031905\n",
    "398447      1           4        1905        0  1041905\n",
    "398448      1           5        1905        1  1051905\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "33d89e6cee0af2c62d42aa05817c4428",
     "grade": false,
     "grade_id": "combine_time_answer",
     "locked": false,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def combine_time(df):\n",
    "    '''\n",
    "    Combines \"Month\", \"DayofMonth\", and \"CRSDepTime\" in the form mmddHHMM.\n",
    "    Creates a new column named \"Time\".\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df: A pandas.DataFrame\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    A pandas.DataFrame\n",
    "    '''\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "875f79bbc91909037f034570097a0e89",
     "grade": false,
     "grade_id": "time_delayed",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "time_delayed = combine_time(local)\n",
    "print(time_delayed.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "b7580bf74f56cd2011935a03a1346a18",
     "grade": true,
     "grade_id": "combine_time_test",
     "locked": true,
     "points": 5,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "df0 = pd.DataFrame({\n",
    "    'Month':      [  1,   2,  10,   12,   9,     7,    5,   11],\n",
    "    'DayofMonth': [  1,  31,   5,   29,  30,     4,   12,   11],\n",
    "    'CRSDepTime': [600, 800, 700,  900, 2300, 1200, 1500, 1800]\n",
    "    })\n",
    "\n",
    "df_answer = df0.join(pd.DataFrame({\n",
    "    'Time': [1010600, 2310800, 10050700, 12290900, 9302300, 7041200, 5121500, 11111800]\n",
    "    }))\n",
    "\n",
    "assert_is_not(combine_time(df0), df0)\n",
    "assert_frame_equal(combine_time(df0), df_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the release of this assignment, a bug has been found by Sean Vig, who also wrote a vectorized version of the function. Here, I first present the original instructions and the original code, and then actually use the improved version by Sean.\n",
    "\n",
    "### Original version\n",
    "\n",
    "Now we find the time closest to the departure time. The following code cell will take a few minutes because we are using `iterrows()`, which is essentially a `for` loop. When you are doing numerical operations with big data in Python, you should avoid for loops as much as possible, and this is why. It's slow. Maybe there's a clever way to do this in a vectorized way, but I couldn't figure it out.\n",
    "\n",
    "You don't have to write the `match_visibility()` function, but you should understand what it's doing.\n",
    "\n",
    "```python\n",
    ">>> def match_visibility(df_delayed, df_visibility, inplace=False):\n",
    "... if not inplace:\n",
    "... # we don't want to change the original data frame\n",
    "...     result = df_delayed.copy()\n",
    "...\n",
    "...     for idx, row in result.iterrows():\n",
    "...     # find the row where the difference between the two times is minimum\n",
    "...         matched = (row['Time'] - df_visibility['Time']).idxmin()\n",
    "...     # used the index we found to extract the visibility and insert it into the result\n",
    "...     result.loc[idx, 'Visibility'] = df_visibility.loc[matched, 'Visibility']\n",
    "...\n",
    "...     return result\n",
    "\n",
    ">>> local_visi = match_visibility(time_delayed, time_visi)\n",
    ">>> print(local_visi.head())\n",
    "```\n",
    "\n",
    "```\n",
    "        Month  DayofMonth  CRSDepTime  Delayed     Time  Visibility\n",
    "398444      1           1        1905        1  1011905          10\n",
    "398445      1           2        1905        1  1021905           9\n",
    "398446      1           3        1905        1  1031905           5\n",
    "398447      1           4        1905        0  1041905           7\n",
    "398448      1           5        1905        1  1051905          10\n",
    "```\n",
    "\n",
    "### Improved version\n",
    "\n",
    "As pointed out by Sean, note we should use `np.abs()`:\n",
    "\n",
    "```python\n",
    "matched = np.abs(row['Time'] - df_visibility['Time']).idxmin()\n",
    "```\n",
    "\n",
    "And the following code cell uses the improved, vectorized version by Sean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "cb2a98ec398988f9c2b9c5f38b551b37",
     "grade": false,
     "grade_id": "match_visibility",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def match_visibility(df_delayed, df_visibility, inplace=False):\n",
    "    if inplace:\n",
    "        # we'll make changes right on df_delayed\n",
    "        result = df_delayed\n",
    "    else:\n",
    "        # we don't want to change the original data frame\n",
    "        result = df_delayed.copy()\n",
    "    \n",
    "    # get the numpy arrays for the flight and visibility times\n",
    "    flight_time = df_delayed['Time'].values\n",
    "    vis_time = df_visibility['Time'].values\n",
    "    \n",
    "    # find the index of first visibility time greater than each flight time\n",
    "    idx = np.searchsorted(vis_time, flight_time)\n",
    "    # constrain to be valid array index > 0\n",
    "    idx = np.clip(idx, 1, len(vis_time) - 1)\n",
    "    \n",
    "    # find index of closest visibility time, either idx or idx-1\n",
    "    # note this comparison will properly handle when a flight time is prior to all visibility readings\n",
    "    # and when the flight time is after all visibility readings\n",
    "    # The `<=` is chosen to match the result of the non-vectorized version\n",
    "    idx = np.where(flight_time - vis_time[idx - 1] <= vis_time[idx] - flight_time,\n",
    "                   idx - 1, idx)\n",
    "    \n",
    "    result['Visibility'] = df_visibility['Visibility'][idx].values\n",
    "    \n",
    "    return result\n",
    "\n",
    "local_visi = match_visibility(time_delayed, time_visi)\n",
    "\n",
    "print(local_visi.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will split the data set into training and test sets. We will train on two columns, `CRSDepTime` and `Visibility`, so let's drop those columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "6d589b18fa6e1611cd7a7ccb1920cd7f",
     "grade": false,
     "grade_id": "local_visi",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "local_visi = local_visi.drop(['Month', 'DayofMonth', 'Time'], axis=1)\n",
    "print(local_visi.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split\n",
    "\n",
    "This function is the same function from [Problem 3.1](https://github.com/UI-DataScience/info490-sp16/blob/master/Week3/assignments/w3p1.ipynb). You can copy-paste your answer. I'll try not to make you write this again in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "44c0df76f1074c13ffd6881436ca5487",
     "grade": false,
     "grade_id": "split_answer",
     "locked": false,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def split(df, test_column, test_size, random_state):\n",
    "    '''\n",
    "    Uses sklearn.train_test_split to split \"df\" into a testing set and a test set.\n",
    "    The \"test_columns\" lists the column that we are trying to predict.\n",
    "    All columns in \"df\" except \"test_columns\" will be used for training.\n",
    "    The \"test_size\" should be between 0.0 and 1.0 and represents the proportion of the\n",
    "    dataset to include in the test split.\n",
    "    The \"random_state\" parameter is used in sklearn.train_test_split.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df: A pandas.DataFrame\n",
    "    test_columns: A list of strings\n",
    "    test_size: A float\n",
    "    random_state: A numpy.random.RandomState instance\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    A 4-tuple of pandas.DataFrames\n",
    "    '''\n",
    "    ##################\n",
    "    # YOUR CODE HERE\n",
    "    ##################\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split `local_visi` into 80:20 training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "6284f66d2068bcb0908580564810c9f0",
     "grade": false,
     "grade_id": "split_run_test",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = split(\n",
    "    df=local_visi,\n",
    "    test_column=['Delayed'],\n",
    "    test_size=0.2,\n",
    "    random_state=check_random_state(0)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following code cell, we test if the returned DataFrames have the correct columns and lengths."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a Decision Trees model\n",
    "\n",
    "- Write a function named `fit_and_predict()` that trains a **Decision Trees** model. Use default parameters. Don't forget that we have to pass an instance of check_random_state() to the train_test_split() function for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "77a6e4f551bc36cea935550ea3fdf9cc",
     "grade": false,
     "grade_id": "fit_and_predict_answer",
     "locked": false,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def fit_and_predict(X_train, y_train, X_test, random_state):\n",
    "    '''\n",
    "    Fits Decision Trees.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X: A pandas.DataFrame. Training attributes.\n",
    "    y: A pandas.DataFrame. Truth labels.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    A numpy array.\n",
    "    '''\n",
    "    ##################\n",
    "    # YOUR CODE HERE\n",
    "    ##################\n",
    "\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "a166bceb72bfea7d0a0f45bdacf3b201",
     "grade": false,
     "grade_id": "y_pred",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "y_pred = fit_and_predict(X_train, y_train, X_test, random_state=check_random_state(0))\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print('The accuracy score is {:0.2f}.'.format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "31d238b986fc5db05ec46f3cdf93d299",
     "grade": true,
     "grade_id": "fit_and_predict_test",
     "locked": true,
     "points": 10,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert_is_instance(y_pred, np.ndarray)\n",
    "assert_equal(len(y_pred), len(y_test))\n",
    "assert_almost_equal(accuracy, 0.817207853501)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}